{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import polars as pl\n",
    "from datetime import timedelta\n",
    "import datetime \n",
    "import json \n",
    "import toml\n",
    "import holidays\n",
    "import sys\n",
    "\n",
    "features = toml.load(r'C:\\Users\\N000193384\\Documents\\sncf_project\\sncf_playground\\data\\features.toml')\n",
    "times_cols = features['times_cols']\n",
    "macro_horizon = features['MACRO_HORIZON']\n",
    "p = Path(features['ABS_DATA_PATH'])\n",
    "sys.path.insert(1, p)\n",
    "\n",
    "from src.preprocessing.times import (\n",
    "    from_day_to_time_fe,\n",
    "    get_covid_table,\n",
    ")\n",
    "from src.preprocessing.quality import trim_timeseries, minimum_length_uid\n",
    "from src.models.forecast.direct import DirectForecaster\n",
    "from src.preprocessing.lags import get_significant_lags\n",
    "from src.preprocessing.times import get_basic_holidays\n",
    "from src.project_utils import load_data\n",
    "from src.models.lgb_wrapper import GBTModel\n",
    "from src.preprocessing.validation import freeze_validation_set\n",
    "from src.preprocessing.lags import compute_autoreg_features\n",
    "\n",
    "ts_uid = features[\"ts_uid\"]\n",
    "date_col = features['date_col']\n",
    "y = features['y']\n",
    "submit = False \n",
    "flist = features[\"flist\"]\n",
    "long_horizon = np.arange(macro_horizon)\n",
    "chains = np.array_split(long_horizon, 6)\n",
    "exog = [\"job\", \"ferie\", \"vacances\"] + times_cols\n",
    "\n",
    "with open('data/params.json', 'rb') as stream:\n",
    "    params_q = json.load(stream)\n",
    "\n",
    "in_dt = datetime.date(2020, 6, 1)\n",
    "\n",
    "covid_df = get_covid_table(2015, 2024)\n",
    "df_dates = df_dates = get_basic_holidays()\n",
    "holidays_fe = list(filter(lambda x: date_col not in x, df_dates.columns))\n",
    "covid_fe = list(filter(lambda x: date_col not in x, covid_df.columns))\n",
    "exog = exog + holidays_fe + covid_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 75) (470988224.py, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[38], line 75\u001b[1;36m\u001b[0m\n\u001b[1;33m    target = \"y\"\"\"\"\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 75)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_data, test_data, submission = load_data(p)\n",
    "\n",
    "test_data = (\n",
    "    test_data.pipe(from_day_to_time_fe, time=date_col, frequency=\"day\")\n",
    "    .join(df_dates, how=\"left\", on=[date_col])\n",
    "    .join(covid_df, how=\"left\", on=[date_col])\n",
    ")\n",
    "\n",
    "train_data = (\n",
    "    train_data.pipe(trim_timeseries, target=\"y\", uid=ts_uid, time=date_col)\n",
    "    .pipe(from_day_to_time_fe, time=date_col, frequency=\"day\")\n",
    "    .join(df_dates, how=\"left\", on=[date_col])\n",
    "    .join(\n",
    "        covid_df.with_columns(\n",
    "            pl.lit(np.where(np.any(covid_df != 0, axis=1), 0.01, 1)).alias(\n",
    "                \"covid_weight\"\n",
    "            )\n",
    "        ),\n",
    "        how=\"left\",\n",
    "        on=[date_col],\n",
    "    )\n",
    "    # sncf strike | 2019-12-01 to 2021-11-01\n",
    "    .filter(\n",
    "        (pl.col(date_col) >= in_dt)\n",
    "        & (pl.col(date_col) != pl.datetime(2019, 12, 1))\n",
    "        & (pl.col(date_col) != pl.datetime(2021, 11, 1))\n",
    "    )\n",
    ")\n",
    "\n",
    "good_ts = minimum_length_uid(\n",
    "    train_data, uid=ts_uid, time=date_col, min_length=round(364 * 1.2)\n",
    ")\n",
    "train_data = train_data.filter(pl.col(ts_uid).is_in(good_ts))\n",
    "# test.\n",
    "left_term = train_data.select([date_col, ts_uid, \"y\"] + exog).with_columns(\n",
    "    pl.lit(1).alias(\"train\")\n",
    ")\n",
    "right_term = test_data.select([date_col, ts_uid, \"y\"] + exog).with_columns(\n",
    "    pl.lit(0).alias(\"train\")\n",
    ")\n",
    "full_data = pl.concat((left_term, right_term), how=\"vertical_relaxed\")\n",
    "del left_term, right_term\n",
    "\n",
    "\n",
    "# define params\n",
    "significant_lags = get_significant_lags(train_data, date_col=date_col, target=y)\n",
    "significant_lags = [x for x in significant_lags if x%7 == 0 and x<= macro_horizon][1:]\n",
    "\n",
    "autoreg_dict = {\n",
    "        ts_uid: {\n",
    "            \"groups\": ts_uid,\n",
    "            \"horizon\": lambda horizon: np.int32(horizon),\n",
    "            \"wins\": np.array([28, 56]),\n",
    "            \"shifts\": lambda horizon: np.int32([horizon]),\n",
    "            \"lags\": lambda horizon: np.array(significant_lags) + horizon,\n",
    "            \"funcs\": np.array(flist),\n",
    "        },\n",
    "        \"ts_uid_dow\": {\n",
    "            \"groups\": [ts_uid, \"day_of_week\"],\n",
    "            \"horizon\": lambda horizon: np.int32(np.ceil(horizon / 7) + 1),\n",
    "            \"wins\": np.array([4, 8]),\n",
    "            \"shifts\": lambda horizon: np.int32([np.ceil(horizon / 7) + 1]),\n",
    "            \"lags\": lambda horizon: np.arange(1, 7) + np.ceil(horizon / 7) + 1,\n",
    "            \"funcs\": np.array(flist),\n",
    "        }\n",
    "    }\n",
    "\n",
    "for key in autoreg_dict.keys():\n",
    "    autoreg_dict[key][\"horizon\"] = autoreg_dict[key][\"horizon\"](macro_horizon)\n",
    "    autoreg_dict[key][\"shifts\"] = autoreg_dict[key][\"shifts\"](macro_horizon)\n",
    "    autoreg_dict[key][\"lags\"] = autoreg_dict[key][\"lags\"](macro_horizon)\n",
    "\n",
    "cat_cols = ['week', 'month', 'day_of_week', \"job\", \"ferie\", \"vacances\"]\n",
    "target = \"y\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def objective(\n",
    "    trial: optuna.trial,\n",
    "    train_data: pd.DataFrame,\n",
    "    exog: List,\n",
    "    seed: int = 12345,\n",
    "):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial): _description_\n",
    "        train_x (pd.DataFrame): _description_\n",
    "        test (pd.DataFrame): _description_\n",
    "        features (List): _description_\n",
    "        target (sself, optional): _description_. Defaults to \"\".\n",
    "        seed (int, optional): _description_. Defaults to 12345.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    optuna_params = {\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\"]),\n",
    "        \"objective\": trial.suggest_categorical(\n",
    "            \"objective\", [\"regression\", \"huber\", \"regression_l1\", \"quantile\"]\n",
    "        ),\n",
    "        \"metric\": trial.suggest_categorical(\"metric\", [\"rmse\"]),\n",
    "        \"alpha\": trial.suggest_categorical(\n",
    "            \"alpha\",\n",
    "            [0.5, 0.52, 0.55, 0.57, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.7, 0.69, 0.7],\n",
    "        ),\n",
    "        \"force_row_wise\": trial.suggest_categorical(\"force_row_wise\", [True, False]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05, log=False),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 15),\n",
    "        \"sub_row\": trial.suggest_categorical(\"sub_row\", [0.6, 0.7, 0.8, 1.0]),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 4, log=True),\n",
    "        \"min_child_samples\": trial.suggest_float(\n",
    "            \"min_child_samples\", 20, 5000, log=False\n",
    "        ),\n",
    "        \"num_iterations\": trial.suggest_int(\n",
    "            \"n_estimators\",\n",
    "            200,\n",
    "            3000,\n",
    "        ),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 25, 800),\n",
    "        \"max_bins\": trial.suggest_int(\"max_bins\", 24, 1000),\n",
    "        \"min_data_in_bin\": trial.suggest_int(\"min_data_in_bin\", 25, 1000),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 1000),\n",
    "        \"feature_fraction_seed\": trial.suggest_categorical(\n",
    "            \"feature_fraction_seed\", [seed]\n",
    "        ),\n",
    "        \"bagging_seed\": trial.suggest_categorical(\"bagging_seed\", [seed]),\n",
    "        \"seed\": trial.suggest_categorical(\"seed\", [seed]),\n",
    "        \"verbose\": trial.suggest_categorical(\"verbose\", [-1]),\n",
    "    }\n",
    "\n",
    "    horizon = 181\n",
    "    lags = [x for x in significant_lags if x <= horizon]\n",
    "    win_list =  [ x for x in significant_lags if x % 7 == 0 and x <= horizon][1:] \n",
    "\n",
    "    autoreg_dict = {\n",
    "        ts_uid : {\n",
    "            'groups' : ts_uid,\n",
    "            'horizon': lambda horizon : np.int32(horizon),\n",
    "            'wins' : np.array(win_list), \n",
    "            'shifts' : lambda horizon : np.int32([horizon, horizon+28, horizon+56]), \n",
    "            'lags' : lambda horizon : np.array(significant_lags) + horizon,\n",
    "            'funcs' : np.array(flist)\n",
    "        },\n",
    "        \"ts_uid_dow\" : {\n",
    "            'groups':[ts_uid, 'day_of_week'],\n",
    "            'horizon' : lambda horizon : np.int32(np.ceil(horizon /7) + 1),\n",
    "            'wins' : np.array([4, 8, 12, 16, 20]), \n",
    "            'shifts' : lambda horizon : np.int32([np.ceil(horizon /7) + 1, np.ceil(horizon /7) + 4]), \n",
    "            'lags' : lambda horizon : np.arange(1, 7) + np.ceil(horizon/7)+1,\n",
    "            'funcs' : np.array(flist)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    effect_m = GBTModel(params=optuna_params, \n",
    "                        early_stopping_value=200, \n",
    "                        features= None,\n",
    "                        custom_loss=optuna_params[\"objective\"], \n",
    "                        categorical_features=[]\n",
    "                        )\n",
    "\n",
    "\n",
    "    dir_forecaster = DirectForecaster(\n",
    "        model=effect_m,\n",
    "        ts_uid=ts_uid,\n",
    "        forecast_range=np.arange(horizon),\n",
    "        target_str=\"y\",\n",
    "        date_str=\"date\",\n",
    "        exogs=exog,\n",
    "        features_params=autoreg_dict\n",
    "    )\n",
    "\n",
    "    dir_forecaster.fit(train_data=train_data)\n",
    "    return dir_forecaster.evaluate()[\"mae\"].values[0]\n",
    "\n",
    "\n",
    "def parameters_tuning(\n",
    "    initial_params: Dict,\n",
    "    tuning_objective,\n",
    "    n_trials: int = 25,\n",
    "    njobs: int = -1,\n",
    "):\n",
    "    \"\"\"parameter for tuning over sudy\n",
    "\n",
    "    Args:\n",
    "        tuning_objective (_type_): _description_\n",
    "        n_trials (int, optional): _description_. Defaults to 25.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "\n",
    "    example :\n",
    "\n",
    "    func = lambda trial: objective(trial=trial,\n",
    "                                    train_x=train_x,\n",
    "                                    test=residualised_test,\n",
    "                                    covariates=covariates,\n",
    "                                    target=y,\n",
    "                                    seed=12345\n",
    "                                    )\n",
    "    study_df, best_params = parameters_tuning(tuning_objective=func, n_trials=25, initial_params={})\n",
    "        print(best_params)\n",
    "        print(study_df)\n",
    "        study_df.to_csv('bparamslgb_new.csv', sep=\"|\", index=False)\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    # study.enqueue_trial(initial_params)\n",
    "    study.optimize(tuning_objective, n_trials=n_trials, n_jobs=njobs)\n",
    "    print(\"Number of finished trials:\", len(study.trials))\n",
    "    print(\"Best trial:\", study.best_trial.params)\n",
    "    study_df = study.trials_dataframe()\n",
    "    return study_df, study.best_params\n",
    "\n",
    "\n",
    "func = lambda trial: objective(trial=trial,\n",
    "                                train_data=train_data,\n",
    "                                exog=exog,\n",
    "                                seed=12345\n",
    "                                )\n",
    "study_df, best_params = parameters_tuning(tuning_objective=func, n_trials=30, initial_params={}, njobs=10)\n",
    "print(best_params)\n",
    "print(study_df)\n",
    "study_df.to_csv('bparamslgb_new.csv', sep=\"|\", index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Union, Optional, Dict\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from pygam import GAM\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "\n",
    "\n",
    "class LogLinear:\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: List[str],\n",
    "        target: str,\n",
    "        ts_uid: str,\n",
    "        n_jobs: int = -1,\n",
    "        frequency: str = \"D\",\n",
    "        use_gam: bool = True,\n",
    "        date_col:str=\"date\",\n",
    "        ndays:int = 364,\n",
    "        horizon:int=181\n",
    "    ) -> None:\n",
    "        self.n_jobs = n_jobs\n",
    "        self.models = {}\n",
    "        self.ts_uid = ts_uid\n",
    "        self.spline_transformer = None\n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        self.date_col = date_col\n",
    "        self.frequency = frequency\n",
    "        self.use_gam = use_gam\n",
    "        self.ndays = ndays\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def basic_features(self, end_date) -> pl.DataFrame:\n",
    "        index = pd.date_range(end_date - timedelta(days=self.ndays), end_date, freq=self.frequency)\n",
    "        dp = DeterministicProcess(\n",
    "            index=index, \n",
    "            constant=True, \n",
    "            order=1, \n",
    "            seasonal=True, \n",
    "            fourier=0, \n",
    "            additional_terms=(), \n",
    "            drop=True\n",
    "            )\n",
    "        return dp\n",
    "\n",
    "    def fit_single(self, y, max_dt):\n",
    "        dp = self.basic_features(end_date=max_dt)\n",
    "        features = dp.in_sample()\n",
    "        size = y.shape[0]\n",
    "        training_obs = size if self.ndays >= size else self.ndays\n",
    "        features = features[-training_obs:]\n",
    "        y = y[-training_obs:]\n",
    "        model = GAM(distribution=\"normal\") if self.use_gam else RidgeCV(cv=4)\n",
    "        model.fit(features, np.log1p(y))\n",
    "        return (model, dp)\n",
    "\n",
    "    def fit(self, data:pl.DataFrame) -> pl.DataFrame:\n",
    "        self.all_keys = data[self.ts_uid].unique().to_list()\n",
    "        results = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self.fit_single)(\n",
    "                y=df[self.target], \n",
    "                max_dt=df[self.date_col].max())\n",
    "            for key, df in data.group_by([self.ts_uid])\n",
    "        )\n",
    "        for key, (model, dp) in zip(data[self.ts_uid].unique(), results):\n",
    "            self.models[key] = (model, dp)\n",
    "\n",
    "    def predict_single(self, key:str)-> pl.DataFrame:\n",
    "        model, dp = self.models[str(key)]\n",
    "        features = dp.out_of_sample(self.horizon)\n",
    "        forecast = np.expm1(model.predict(features))\n",
    "        output = pl.DataFrame()\n",
    "        output = output.with_columns(\n",
    "            pl.lit(forecast).alias(\"y_hat\"),\n",
    "            pl.lit(key[0]).alias(self.ts_uid),\n",
    "            # pl.lit(X[\"date\"]),\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def predict(self) -> pl.DataFrame:\n",
    "        predictions = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self.predict_single)(key) for key in self.all_keys\n",
    "        )\n",
    "        results = pl.concat(predictions)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeterministicProcess' object has no attribute 'out_sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\N000193384\\AppData\\Local\\Temp\\ipykernel_32224\\1336509744.py\", line 76, in predict_single\nAttributeError: 'DeterministicProcess' object has no attribute 'out_sample'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m ll \u001b[38;5;241m=\u001b[39m LogLinear(\n\u001b[0;32m      2\u001b[0m     features\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mferie\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvacances\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      3\u001b[0m     target\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m181\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m ll\u001b[38;5;241m.\u001b[39mfit(train_data)\n\u001b[1;32m---> 11\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[95], line 87\u001b[0m, in \u001b[0;36mLogLinear.predict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m---> 87\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_single\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_keys\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     results \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat(predictions)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DeterministicProcess' object has no attribute 'out_sample'"
     ]
    }
   ],
   "source": [
    "ll = LogLinear(\n",
    "    features=['job', 'ferie', 'vacances'],\n",
    "    target=y,\n",
    "    ts_uid=ts_uid,\n",
    "    use_gam=True,\n",
    "    ndays=364*2, \n",
    "    horizon=181\n",
    ")\n",
    "\n",
    "ll.fit(train_data)\n",
    "out = ll.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>trend</th>\n",
       "      <th>s(2,7)</th>\n",
       "      <th>s(3,7)</th>\n",
       "      <th>s(4,7)</th>\n",
       "      <th>s(5,7)</th>\n",
       "      <th>s(6,7)</th>\n",
       "      <th>s(7,7)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-02</th>\n",
       "      <td>1.0</td>\n",
       "      <td>731.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03</th>\n",
       "      <td>1.0</td>\n",
       "      <td>732.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-04</th>\n",
       "      <td>1.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-05</th>\n",
       "      <td>1.0</td>\n",
       "      <td>734.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>906.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>907.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>909.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            const  trend  s(2,7)  s(3,7)  s(4,7)  s(5,7)  s(6,7)  s(7,7)\n",
       "2023-01-01    1.0  730.0     1.0     0.0     0.0     0.0     0.0     0.0\n",
       "2023-01-02    1.0  731.0     0.0     1.0     0.0     0.0     0.0     0.0\n",
       "2023-01-03    1.0  732.0     0.0     0.0     1.0     0.0     0.0     0.0\n",
       "2023-01-04    1.0  733.0     0.0     0.0     0.0     1.0     0.0     0.0\n",
       "2023-01-05    1.0  734.0     0.0     0.0     0.0     0.0     1.0     0.0\n",
       "...           ...    ...     ...     ...     ...     ...     ...     ...\n",
       "2023-06-26    1.0  906.0     0.0     1.0     0.0     0.0     0.0     0.0\n",
       "2023-06-27    1.0  907.0     0.0     0.0     1.0     0.0     0.0     0.0\n",
       "2023-06-28    1.0  908.0     0.0     0.0     0.0     1.0     0.0     0.0\n",
       "2023-06-29    1.0  909.0     0.0     0.0     0.0     0.0     1.0     0.0\n",
       "2023-06-30    1.0  910.0     0.0     0.0     0.0     0.0     0.0     1.0\n",
       "\n",
       "[181 rows x 8 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.models.get('BKJ')[1].out_of_sample(181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_col = [ts_uid, date_col, y, \"day_of_year\"]\n",
    "\n",
    "from src.preprocessing.lags import reference_shift_from_day\n",
    "from src.analysis.metrics import display_metrics\n",
    "\n",
    "train_data, test_data, submission = load_data(p)\n",
    "\n",
    "\n",
    "def freeze_validation_set(\n",
    "    df: pl.DataFrame,\n",
    "    date: str,\n",
    "    val_size: int,\n",
    "    return_train: bool = True,\n",
    ") -> pl.DataFrame:\n",
    "    max_dt = df[date].max()\n",
    "    cut = max_dt - timedelta(days=val_size)\n",
    "    valid = df.filter(pl.col(date) > cut)  # .select([ts_uid, date, target])\n",
    "    if return_train:\n",
    "        train = df.filter(pl.col(date) <= cut)\n",
    "        return train, valid\n",
    "    else:\n",
    "        return valid\n",
    "    \n",
    "\n",
    "baseline = (train_data\n",
    "    .pipe(from_day_to_time_fe, time=\"date\", frequency=\"day\")\n",
    "    .pipe(reference_shift_from_day,    \n",
    "        target_col=y, \n",
    "        ts_uid=ts_uid,\n",
    "        dayofyear_col=\"day_of_year\"       )\n",
    "    .pipe(freeze_validation_set, return_train=False, date=date_col, val_size=181)\n",
    "    .with_columns(\n",
    "        pl.coalesce(\n",
    "            pl.col('reference_y'), \n",
    "            pl.col('reference_y').mean().over(ts_uid),\n",
    "            pl.col('reference_y').mean(),\n",
    "            0\n",
    "            )\n",
    "    )\n",
    "    .select([\"index\", date_col, ts_uid, y, \"reference_y\"])\n",
    ")\n",
    "\n",
    "del train_data, test_data\n",
    "import gc\n",
    "gc.collect()\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195307, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direct_mean_global_valid = (pl.read_csv('out/global_log_chain_lgb.csv', separator=\",\", infer_schema_length=1000)\n",
    "                                                 .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "direct_mean_global_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 12, 31)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline[date_col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>station</th><th>date</th><th>global_log_chain_y_hat</th></tr><tr><td>str</td><td>date</td><td>f64</td></tr></thead><tbody><tr><td>&quot;OYD&quot;</td><td>2022-12-28</td><td>52.332357</td></tr><tr><td>&quot;OYD&quot;</td><td>2022-12-28</td><td>58.888227</td></tr><tr><td>&quot;OYD&quot;</td><td>2022-12-28</td><td>70.471682</td></tr><tr><td>&quot;OYD&quot;</td><td>2022-12-28</td><td>97.832968</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 3)\n",
       "┌─────────┬────────────┬────────────────────────┐\n",
       "│ station ┆ date       ┆ global_log_chain_y_hat │\n",
       "│ ---     ┆ ---        ┆ ---                    │\n",
       "│ str     ┆ date       ┆ f64                    │\n",
       "╞═════════╪════════════╪════════════════════════╡\n",
       "│ OYD     ┆ 2022-12-28 ┆ 52.332357              │\n",
       "│ OYD     ┆ 2022-12-28 ┆ 58.888227              │\n",
       "│ OYD     ┆ 2022-12-28 ┆ 70.471682              │\n",
       "│ OYD     ┆ 2022-12-28 ┆ 97.832968              │\n",
       "└─────────┴────────────┴────────────────────────┘"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direct_mean_global_valid.filter((pl.col(ts_uid) == \"OYD\") & (pl.col(date_col) == datetime.datetime(2022, 12, 28))).sort(by=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_direct_mlp.csv\n",
      "global_log_chain_lgb.csv\n",
      "(78102, 5)\n",
      "global_log_direct_lgb.csv\n",
      "(78102, 6)\n",
      "global_mean_chain_lgb.csv\n",
      "(78102, 7)\n",
      "global_mean_direct_lgb.csv\n",
      "(78102, 8)\n",
      "global_median_chain_lgb.csv\n",
      "(78102, 9)\n",
      "global_median_direct_lgb.csv\n",
      "(78102, 10)\n",
      "global_None_chain_lgb.csv\n",
      "(78102, 11)\n",
      "global_None_direct_lgb.csv\n",
      "(78102, 12)\n",
      "global_rolling_mean_chain_lgb.csv\n",
      "(78102, 13)\n",
      "global_rolling_mean_direct_lgb.csv\n",
      "(78102, 14)\n",
      "global_rolling_median_chain_lgb.csv\n",
      "(78102, 15)\n",
      "global_rolling_median_direct_lgb.csv\n",
      "(78102, 16)\n",
      "global_rolling_zscore_chain_lgb.csv\n",
      "(78102, 17)\n",
      "global_rolling_zscore_direct_lgb.csv\n",
      "(78102, 18)\n",
      "global_shiftn_chain_lgb.csv\n",
      "(78102, 19)\n",
      "global_shiftn_direct_lgb.csv\n",
      "(78102, 20)\n",
      "local_log_direct_lgb.csv\n",
      "(78102, 21)\n"
     ]
    },
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"local_log_direct_y_hat\"; valid columns: [\"date\", \"station\", \"job\", \"ferie\", \"vacances\", \"index\", \"is_train\", \"y\", \"y_copy\", \"not_outlier\", \"year\", \"week\", \"month\", \"day\", \"day_of_year\", \"day_of_week\", \"quarter\", \"week_of_month\", \"is_weekend\", \"New Year's Day\", \"Easter Monday\", \"Labor Day\", \"Victory Day\", \"Ascension Day\", \"Whit Monday\", \"National Day\", \"Assumption Day\", \"All Saints' Day\", \"Armistice Day\", \"Christmas Day\", \"First Lockdown\", \"Second Lockdown\", \"Third Lockdown (Partial)\", \"covid_weight\", \"ar_y_lag181_station\", \"ar_y_lag188_station\", \"ar_y_lag195_station\", \"ar_y_lag202_station\", \"ar_y_lag209_station\", \"ar_y_lag216_station\", \"ar_y_lag230_station\", \"ar_y_lag272_station\", \"ar_y_lag300_station\", \"ar_y_win7_shift181_rolling_mean_station\", \"ar_y_win7_shift181_rolling_median_station\", \"ar_y_win7_shift181_rolling_std_station\", \"ar_y_win7_shift181_rolling_skew_station\", \"ar_y_win7_shift209_rolling_mean_station\", \"ar_y_win7_shift209_rolling_median_station\", \"ar_y_win7_shift209_rolling_std_station\", \"ar_y_win7_shift209_rolling_skew_station\", \"ar_y_win28_shift181_rolling_mean_station\", \"ar_y_win28_shift181_rolling_median_station\", \"ar_y_win28_shift181_rolling_std_station\", \"ar_y_win28_shift181_rolling_skew_station\", \"ar_y_win28_shift209_rolling_mean_station\", \"ar_y_win28_shift209_rolling_median_station\", \"ar_y_win28_shift209_rolling_std_station\", \"ar_y_win28_shift209_rolling_skew_station\", \"ar_y_win56_shift181_rolling_mean_station\", \"ar_y_win56_shift181_rolling_median_station\", \"ar_y_win56_shift181_rolling_std_station\", \"ar_y_win56_shift181_rolling_skew_station\", \"ar_y_win56_shift209_rolling_mean_station\", \"ar_y_win56_shift209_rolling_median_station\", \"ar_y_win56_shift209_rolling_std_station\", \"ar_y_win56_shift209_rolling_skew_station\", \"ar_y_lag28.0_station@day_of_week\", \"ar_y_lag29.0_station@day_of_week\", \"ar_y_lag30.0_station@day_of_week\", \"ar_y_lag31.0_station@day_of_week\", \"ar_y_lag32.0_station@day_of_week\", \"ar_y_lag33.0_station@day_of_week\", \"ar_y_win4_shift27_rolling_mean_station@day_of_week\", \"ar_y_win4_shift27_rolling_median_station@day_of_week\", \"ar_y_win4_shift27_rolling_std_station@day_of_week\", \"ar_y_win4_shift27_rolling_skew_station@day_of_week\", \"ar_y_win4_shift71_rolling_mean_station@day_of_week\", \"ar_y_win4_shift71_rolling_median_station@day_of_week\", \"ar_y_win4_shift71_rolling_std_station@day_of_week\", \"ar_y_win4_shift71_rolling_skew_station@day_of_week\", \"ar_y_win12_shift27_rolling_mean_station@day_of_week\", \"ar_y_win12_shift27_rolling_median_station@day_of_week\", \"ar_y_win12_shift27_rolling_std_station@day_of_week\", \"ar_y_win12_shift27_rolling_skew_station@day_of_week\", \"ar_y_win12_shift71_rolling_mean_station@day_of_week\", \"ar_y_win12_shift71_rolling_median_station@day_of_week\", \"ar_y_win12_shift71_rolling_std_station@day_of_week\", \"ar_y_win12_shift71_rolling_skew_station@day_of_week\", \"local_direct_log_y_hat\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mColumnNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m fcst_col \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_hat\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(baseline\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      9\u001b[0m baseline \u001b[38;5;241m=\u001b[39m baseline\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m---> 10\u001b[0m     (\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfiles\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mts_uid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlgb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_hat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(date_col)\u001b[38;5;241m.\u001b[39mcast(pl\u001b[38;5;241m.\u001b[39mDate)\u001b[38;5;241m.\u001b[39malias(date_col))\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39munique(subset\u001b[38;5;241m=\u001b[39m[ts_uid, date_col])\n\u001b[0;32m     16\u001b[0m     ), \n\u001b[0;32m     17\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, on\u001b[38;5;241m=\u001b[39m[ts_uid, date_col]\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m fcol\u001b[38;5;241m.\u001b[39mappend(fcst_col)\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\_utils\\deprecation.py:135\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    132\u001b[0m     _rename_keyword_argument(\n\u001b[0;32m    133\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, version\n\u001b[0;32m    134\u001b[0m     )\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\_utils\\deprecation.py:135\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    132\u001b[0m     _rename_keyword_argument(\n\u001b[0;32m    133\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, version\n\u001b[0;32m    134\u001b[0m     )\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\_utils\\deprecation.py:135\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    132\u001b[0m     _rename_keyword_argument(\n\u001b[0;32m    133\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, version\n\u001b[0;32m    134\u001b[0m     )\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\io\\csv\\functions.py:419\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, dtypes, schema, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[0;32m    407\u001b[0m         dtypes \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    408\u001b[0m             new_to_current\u001b[38;5;241m.\u001b[39mget(column_name, column_name): column_dtype\n\u001b[0;32m    409\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m column_name, column_dtype \u001b[38;5;129;01min\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    410\u001b[0m         }\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m prepare_file_arg(\n\u001b[0;32m    413\u001b[0m     source,\n\u001b[0;32m    414\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    418\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[1;32m--> 419\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43m_read_csv_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnull_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8-lossy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43meol_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_columns:\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _update_columns(df, new_columns)\n",
      "File \u001b[1;32mc:\\Users\\N000193384\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\io\\csv\\functions.py:565\u001b[0m, in \u001b[0;36m_read_csv_impl\u001b[1;34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, dtypes, schema, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    563\u001b[0m projection, columns \u001b[38;5;241m=\u001b[39m parse_columns_arg(columns)\n\u001b[1;32m--> 565\u001b[0m pydf \u001b[38;5;241m=\u001b[39m \u001b[43mPyDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_null_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_row_index_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43meol_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(pydf)\n",
      "\u001b[1;31mColumnNotFoundError\u001b[0m: unable to find column \"local_log_direct_y_hat\"; valid columns: [\"date\", \"station\", \"job\", \"ferie\", \"vacances\", \"index\", \"is_train\", \"y\", \"y_copy\", \"not_outlier\", \"year\", \"week\", \"month\", \"day\", \"day_of_year\", \"day_of_week\", \"quarter\", \"week_of_month\", \"is_weekend\", \"New Year's Day\", \"Easter Monday\", \"Labor Day\", \"Victory Day\", \"Ascension Day\", \"Whit Monday\", \"National Day\", \"Assumption Day\", \"All Saints' Day\", \"Armistice Day\", \"Christmas Day\", \"First Lockdown\", \"Second Lockdown\", \"Third Lockdown (Partial)\", \"covid_weight\", \"ar_y_lag181_station\", \"ar_y_lag188_station\", \"ar_y_lag195_station\", \"ar_y_lag202_station\", \"ar_y_lag209_station\", \"ar_y_lag216_station\", \"ar_y_lag230_station\", \"ar_y_lag272_station\", \"ar_y_lag300_station\", \"ar_y_win7_shift181_rolling_mean_station\", \"ar_y_win7_shift181_rolling_median_station\", \"ar_y_win7_shift181_rolling_std_station\", \"ar_y_win7_shift181_rolling_skew_station\", \"ar_y_win7_shift209_rolling_mean_station\", \"ar_y_win7_shift209_rolling_median_station\", \"ar_y_win7_shift209_rolling_std_station\", \"ar_y_win7_shift209_rolling_skew_station\", \"ar_y_win28_shift181_rolling_mean_station\", \"ar_y_win28_shift181_rolling_median_station\", \"ar_y_win28_shift181_rolling_std_station\", \"ar_y_win28_shift181_rolling_skew_station\", \"ar_y_win28_shift209_rolling_mean_station\", \"ar_y_win28_shift209_rolling_median_station\", \"ar_y_win28_shift209_rolling_std_station\", \"ar_y_win28_shift209_rolling_skew_station\", \"ar_y_win56_shift181_rolling_mean_station\", \"ar_y_win56_shift181_rolling_median_station\", \"ar_y_win56_shift181_rolling_std_station\", \"ar_y_win56_shift181_rolling_skew_station\", \"ar_y_win56_shift209_rolling_mean_station\", \"ar_y_win56_shift209_rolling_median_station\", \"ar_y_win56_shift209_rolling_std_station\", \"ar_y_win56_shift209_rolling_skew_station\", \"ar_y_lag28.0_station@day_of_week\", \"ar_y_lag29.0_station@day_of_week\", \"ar_y_lag30.0_station@day_of_week\", \"ar_y_lag31.0_station@day_of_week\", \"ar_y_lag32.0_station@day_of_week\", \"ar_y_lag33.0_station@day_of_week\", \"ar_y_win4_shift27_rolling_mean_station@day_of_week\", \"ar_y_win4_shift27_rolling_median_station@day_of_week\", \"ar_y_win4_shift27_rolling_std_station@day_of_week\", \"ar_y_win4_shift27_rolling_skew_station@day_of_week\", \"ar_y_win4_shift71_rolling_mean_station@day_of_week\", \"ar_y_win4_shift71_rolling_median_station@day_of_week\", \"ar_y_win4_shift71_rolling_std_station@day_of_week\", \"ar_y_win4_shift71_rolling_skew_station@day_of_week\", \"ar_y_win12_shift27_rolling_mean_station@day_of_week\", \"ar_y_win12_shift27_rolling_median_station@day_of_week\", \"ar_y_win12_shift27_rolling_std_station@day_of_week\", \"ar_y_win12_shift27_rolling_skew_station@day_of_week\", \"ar_y_win12_shift71_rolling_mean_station@day_of_week\", \"ar_y_win12_shift71_rolling_median_station@day_of_week\", \"ar_y_win12_shift71_rolling_std_station@day_of_week\", \"ar_y_win12_shift71_rolling_skew_station@day_of_week\", \"local_direct_log_y_hat\"]"
     ]
    }
   ],
   "source": [
    "all_files = list(filter(lambda s : \".csv\" in s, os.listdir('out/')))\n",
    "fcol = []\n",
    "for files in all_files:\n",
    "    print(files)\n",
    "    if files not in ['global_direct_mlp.csv', 'nixtla_validation.csv', \n",
    "                     'prophet.csv', 'neural_validation.csv', 'mlp_forecast.csv']:\n",
    "        fcst_col = files.replace('lgb', 'y_hat').replace('.csv', '')\n",
    "        print(baseline.shape)\n",
    "        baseline = baseline.join(\n",
    "            (pl.read_csv(f'out/{files}', \n",
    "                        separator=\",\", \n",
    "                        infer_schema_length=1000,\n",
    "                        columns=[ts_uid, date_col, files.replace('lgb', 'y_hat').replace('.csv', '')]\n",
    "            ).with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    "            .unique(subset=[ts_uid, date_col])\n",
    "            ), \n",
    "            how=\"left\", on=[ts_uid, date_col]\n",
    "        )\n",
    "        fcol.append(fcst_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "statsmodel_valid = (pl.read_csv('out/nixtla_validation.csv', separator=\",\", infer_schema_length=1000)\n",
    "                    .rename({\"unique_id\":ts_uid, \"ds\":date_col})\n",
    "                    .select([ts_uid, date_col, 'HoltWinters', 'AutoETS', \"AutoARIMA\",\n",
    "                              'AutoTheta', 'AutoTBATS', 'arithmetic_forecast_ensamble']\n",
    "                              ).with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    ")\n",
    "\n",
    "\n",
    "direct_mean_global_valid = (pl.read_csv('out/global_mean_direct_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                        columns=[ts_uid, date_col, \"global_direct_mean_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "direct_rolling_zscore_global_valid = (pl.read_csv('out/global_rolling_zscore_direct_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                                  columns=[ts_uid, date_col, \"global_direct_rolling_zscore_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "direct_med_global_valid = (pl.read_csv('out/global_median_direct_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                       columns=[ts_uid, date_col, \"global_direct_median_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "direct_None_global_valid = (pl.read_csv('out/global_None_direct_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                        columns=[ts_uid, date_col, \"global_direct_None_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "direct_log_global_valid = (pl.read_csv('out/global_log_direct_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                       columns=[ts_uid, date_col, \"global_direct_log_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "direct_rolmean_global_valid = (pl.read_csv('out/global_rolling_mean_direct_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                           columns=[ts_uid, date_col, \"global_direct_rolling_mean_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "direct_rolmed_global_valid = (pl.read_csv('out/global_rolling_median_direct_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                          columns=[ts_uid, date_col, \"global_direct_rolling_median_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "direct_shiftn_global_valid = (pl.read_csv('out/global_shiftn_direct_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                          columns=[ts_uid, date_col, \"global_direct_shiftn_y_hat\"]\n",
    ")\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "# =======\n",
    "chain_mean_global_valid = (pl.read_csv('out/global_mean_chain_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                       columns=[ts_uid, date_col, \"global_mean_chain_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "chain_rolling_zscore_global_valid = (pl.read_csv('out/global_rolling_zscore_chain_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                                 columns=[ts_uid, date_col, \"global_rolling_zscore_chain_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "chain_med_global_valid = (pl.read_csv('out/global_median_chain_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                      columns=[ts_uid, date_col, \"global_median_chain_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "chain_None_global_valid = (pl.read_csv('out/global_None_chain_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                       columns=[ts_uid, date_col, \"global_None_chain_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "chain_log_global_valid = (pl.read_csv('out/global_log_chain_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                      columns=[ts_uid, date_col, \"global_log_chain_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "chain_rolmean_global_valid = (pl.read_csv('out/global_rolling_mean_chain_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                          columns=[ts_uid, date_col, \"global_rolling_mean_chain_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "chain_rolmed_global_valid = (pl.read_csv('out/global_rolling_median_chain_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                         columns=[ts_uid, date_col, \"global_rolling_median_chain_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "chain_shiftn_global_valid = (pl.read_csv('out/global_shiftn_chain_lgb.csv', separator=\",\", infer_schema_length=1000, \n",
    "                                         columns=[ts_uid, date_col, \"global_shiftn_chain_y_hat\"])\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col),\n",
    "                                   # pl.col(\"global_direct_y_hat\").exp()\n",
    "                                   )\n",
    ")\n",
    "\n",
    "#==============\n",
    "\n",
    "\n",
    "nfcst  = [\"TimesNet\"\t,\"NHITS\"\t,\"NBEATSx\"\t, \"PatchTST\"\t,\"TSMixerx\"\t, \"iTransformer\", \"ensamble_forecast\"]\n",
    "neural_fcst_model = pl.read_csv(\"out/neural_validation.csv\", separator=\",\", infer_schema_length=1000) \n",
    "for col in nfcst:\n",
    "    neural_fcst_model = neural_fcst_model.with_columns(pl.lit(np.expm1(neural_fcst_model[col])).alias(col))\n",
    "neural_fcst_model = neural_fcst_model.rename({\"unique_id\":ts_uid, \"ds\":date_col, \"ensamble_forecast\":\"deep_ensemble\"})\n",
    "neural_fcst_model = neural_fcst_model.with_columns(pl.col(date_col).cast(pl.Date))\n",
    "nfcst = nfcst[:-1] + [\"deep_ensemble\"]\n",
    "\n",
    "direct_local_valid = (pl.read_csv('out/local_direct_lgb.csv', separator=\",\", infer_schema_length=1000)\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    ")\n",
    "\n",
    "\n",
    "prophet_m = (pl.read_csv('out/prophet.csv', infer_schema_length=1000)\n",
    "                     .with_columns(pl.col(\"ds\").str.to_datetime().cast(pl.Date).alias(date_col))\n",
    "                     .rename({\"unique_id\":ts_uid, \"yhat\":\"prophet_yhat\"})\n",
    ")\n",
    "\n",
    "prophet_m = prophet_m.with_columns(pl.lit(np.expm1(prophet_m['prophet_yhat'])))\n",
    "\n",
    "\n",
    "mlp_global_valid  = (pl.read_csv('out/global_direct_mlp.csv', separator=\",\", infer_schema_length=1000)\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    "                     .rename({\"global_direct_y_hat\":\"global_direct_mlp_y_hat\"})\n",
    ")\n",
    "mlp_global_valid = mlp_global_valid.with_columns(pl.lit(np.expm1(mlp_global_valid['global_direct_mlp_y_hat'])))\n",
    "\n",
    "chain_global_valid = (pl.read_csv('out/global_chain_lgb.csv', separator=\",\", infer_schema_length=1000)\n",
    "                     # .rename({\"y_hat\":\"global_chain_y_hat\"})\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    ")\n",
    "\n",
    "baseline = (train_data\n",
    "    .pipe(from_day_to_time_fe, time=\"date\", frequency=\"day\")\n",
    "    .pipe(reference_shift_from_day,    \n",
    "        target_col=y, \n",
    "        ts_uid=ts_uid,\n",
    "        dayofyear_col=\"day_of_year\"       )\n",
    "    .pipe(freeze_validation_set, return_train=False, date=date_col, val_size=181)\n",
    "    .with_columns(\n",
    "        pl.coalesce(\n",
    "            pl.col('reference_y'), \n",
    "            pl.col('reference_y').mean().over(ts_uid),\n",
    "            pl.col('reference_y').mean(),\n",
    "            0\n",
    "            )\n",
    "    )\n",
    "    .join( statsmodel_valid, how=\"left\",  on=[ts_uid, date_col]    )\n",
    "    \"\"\"\n",
    "    .join(\n",
    "            direct_log_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            direct_mean_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            direct_med_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            direct_rolmean_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            direct_rolmed_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            direct_rolling_zscore_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            direct_shiftn_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            direct_None_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    \"\"\"\n",
    "    #=\n",
    "    .join(\n",
    "            chain_mean_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            chain_med_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            chain_rolmean_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            chain_rolmed_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            chain_rolling_zscore_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            chain_shiftn_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            chain_None_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            chain_log_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "\n",
    "    .join(mlp_global_valid.select([ts_uid, date_col, 'global_direct_mlp_y_hat']), how=\"left\", on = [ts_uid, date_col])\n",
    "    .join(neural_fcst_model, how=\"left\", on = [ts_uid, date_col])\n",
    "    .join(prophet_m, how=\"left\", on = [ts_uid, date_col])\n",
    "    )\n",
    "\n",
    "forecast_cols = [\n",
    "                'HoltWinters', 'AutoETS', \"AutoARIMA\",\n",
    "                 'AutoTheta', 'AutoTBATS', \"prophet_yhat\",\n",
    "                 'arithmetic_forecast_ensamble',\n",
    "                 \"reference_y\",\n",
    "\n",
    "                 \"global_direct_mlp_y_hat\", \n",
    "                \n",
    "                 \n",
    "                 # \"global_direct_mean_y_hat\", \"global_direct_shiftn_y_hat\",\n",
    "                 # \"global_direct_median_y_hat\", \"global_direct_rolling_mean_y_hat\", \"global_direct_rolling_median_y_hat\", \n",
    "                 # \"global_direct_None_y_hat\", \"global_direct_log_y_hat\", \"global_direct_rolling_zscore_y_hat\",\n",
    "                 \n",
    "                 \n",
    "                 \"global_chain_mlp_y_hat\", \"global_mean_chain_y_hat\", \"global_shiftn_chain_y_hat\",\n",
    "                 \"global_median_chain_y_hat\", \"global_rolling_median_chain_y_hat\", \"global_rolling_mean_chain_y_hat\", \n",
    "                 \"global_None_chain_y_hat\", \"global_None_chain_y_hat\", \"global_rolling_zscore_chain_y_hat\"\n",
    "                 ] + nfcst\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"baseline = baseline.with_columns(\n",
    "    pl.concat_list([\"reference_y\", \"global_direct_y_hat\", \"global_chain_y_hat\"]).list.mean().alias('ensemble'),\n",
    "    pl.concat_list(forecast_cols).list.mean().alias('all_ensemble'),\n",
    "    pl.concat_list([\"global_direct_y_hat\", \"global_chain_y_hat\"]).list.mean().alias('lgb_ensemble'),                   \n",
    ")\"\"\"\n",
    "\n",
    "# forecast_cols += [\"ensemble\", \"all_ensemble\", \"lgb_ensemble\"]\n",
    "\n",
    "metrics_output_df = []\n",
    "for col in forecast_cols:\n",
    "    metrics_output_df.append(\n",
    "        display_metrics(\n",
    "        baseline[y].to_numpy(), \n",
    "        np.clip(baseline[col].fill_null(0).to_numpy(), a_min=0, a_max=None),\n",
    "        name=col\n",
    "        ).transpose()\n",
    "    )\n",
    "\n",
    "allmet = pd.concat(metrics_output_df, axis=1)\n",
    "allmet.columns = allmet.iloc[0, :]\n",
    "display(allmet.T.style.highlight_min(color=\"blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsse_df = allmet.loc['rmse'] \n",
    "rmsse_df /= rmsse_df['reference_y']\n",
    "rmsse_df = rmsse_df.to_frame('rmsse').reset_index()\n",
    "rmsse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rmsse_df = rmsse_df.loc[~rmsse_df[\"fname\"].isin(['reference_y', 'all_ensemble', 'ensemble', 'lgb_ensemble', \"local_direct_y_hat\", \"NBEATSx\", \"NHITS\"])]\n",
    "rmsse_df['fname'] = np.where(rmsse_df['fname'] == \"arithmetic_forecast_ensamble\", \"statistical_ensemble\", rmsse_df['fname'])\n",
    "rmsse_df['fname'] = np.where(rmsse_df['fname'] == \"global_direct_y_hat\", \"LightGBM Direct Forecast\", rmsse_df['fname'])\n",
    "rmsse_df['fname'] = np.where(rmsse_df['fname'] == \"global_chain_y_hat\", \"LightGBM Chained Forecast\", rmsse_df['fname'])\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "sns.barplot(rmsse_df.sort_values(by=\"rmsse\"), x=\"rmsse\", y=\"fname\",\n",
    "            errorbar=(\"pi\", 50), capsize=.4,\n",
    "    err_kws={\"color\": \".5\", \"linewidth\": 2.5},\n",
    "    linewidth=2.5, edgecolor=\".5\",\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "\n",
    "plt.axvline(1.0, label=\"Baseline\", color=\"black\", linestyle='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 5))\n",
    "sns.barplot(rmsse_df.loc[rmsse_df.index.isin(list(filter(lambda x : \"global\" in x, rmsse_df.index)))].sort_values(by=\"rmsse\"), x=\"rmsse\", y=\"fname\",\n",
    "            errorbar=(\"pi\", 50), capsize=.4,\n",
    "    err_kws={\"color\": \".5\", \"linewidth\": 2.5},\n",
    "    linewidth=2.5, edgecolor=\".5\",\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "\n",
    "plt.axvline(1.0, label=\"Baseline\", color=\"black\", linestyle='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = baseline['date'].unique()\n",
    "split_dt = np.array_split(dt, 5)\n",
    "bycut = []\n",
    "for sdt in split_dt:\n",
    "    metrics_output_df = []\n",
    "    b_cut = baseline.filter(pl.col(\"date\").is_in(sdt))\n",
    "    for col in forecast_cols:\n",
    "        metrics_output_df.append(\n",
    "            display_metrics(\n",
    "            baseline[y].to_numpy(), \n",
    "            np.clip(baseline[col].fill_null(0).to_numpy(), a_min=0, a_max=None),\n",
    "            name=col\n",
    "            ).transpose()\n",
    "        )\n",
    "\n",
    "    allmet = pd.concat(metrics_output_df, axis=1)\n",
    "    allmet.columns = allmet.iloc[0, :]\n",
    "    rmsse_df = allmet.loc['rmse'] \n",
    "    rmsse_df /= rmsse_df['reference_y']\n",
    "    rmsse_df = rmsse_df.to_frame('rmsse')\n",
    "    rmsse_df['date_range'] = str(b_cut['date'].min()) + ' - ' + str(b_cut['date'].max())\n",
    "    # display(allmet)\n",
    "    bycut.append(rmsse_df)\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_ = pd.concat(bycut).reset_index()\n",
    "data_ = data_.loc[~data_[\"fname\"].isin(['reference_y', 'all_ensemble', 'ensemble', 'lgb_ensemble', \"local_direct_y_hat\", \"NBEATSx\", \"NHITS\"])]\n",
    "data_['fname'] = np.where(data_['fname'] == \"arithmetic_forecast_ensamble\", \"statistical_ensemble\", data_['fname'])\n",
    "data_['fname'] = np.where(data_['fname'] == \"global_direct_y_hat\", \"LightGBM Direct Forecast\", data_['fname'])\n",
    "data_['fname'] = np.where(data_['fname'] == \"global_chain_y_hat\", \"LightGBM Chained Forecast\", data_['fname'])\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "sns.barplot(data_.sort_values(by=\"rmsse\"), x=\"date_range\", y=\"rmsse\", hue=\"fname\",     errorbar=(\"pi\", 50), capsize=.4,\n",
    "    err_kws={\"color\": \".5\", \"linewidth\": 2.5},\n",
    "    linewidth=2.5, edgecolor=\".5\",\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "\n",
    "plt.axhline(1.0, label=\"Baseline\", color=\"black\", linestyle='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "uids = baseline['station'].unique()\n",
    "n = 6\n",
    "choice = np.random.choice(uids, (n, ))\n",
    "\n",
    "# Création de la figure et des sous-graphiques\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 16))\n",
    "axs = axs.ravel()  # Aplatir le tableau 2D des axes pour itération facile\n",
    "\n",
    "# Tracer les graphiques\n",
    "for i, uid in enumerate(uids, start=0):\n",
    "    if i  < n :\n",
    "        subset_tr = train_data.filter(\n",
    "            pl.col(ts_uid) == uid, \n",
    "            pl.col('date') >= pl.datetime(2021, 1, 1), \n",
    "            pl.col('date') < subset['date'].min()\n",
    "            ).sort(by='date')\n",
    "        subset = baseline.filter(pl.col(ts_uid) == uid).sort(by='date')\n",
    "        axs[i].plot(subset_tr[\"date\"], subset_tr['y_copy'], label=\"train\", color=\"brown\", alpha=0.8)\n",
    "        axs[i].scatter(subset[\"date\"], subset['y'], label=\"real\", color=\"blue\", alpha=1, marker=\"*\")\n",
    "        for col in list(filter(lambda x : 'y_hat' in x ,forecast_cols)):\n",
    "            axs[i].scatter(subset[\"date\"], subset[col], label=col, alpha=0.5, marker=\"x\")\n",
    "        axs[i].axvline(subset['date'].min())\n",
    "        axs[i].set_title(f'Graph for ID {uid}')\n",
    "        axs[i].legend()\n",
    "\n",
    "# Ajuster l'espacement entre les sous-graphiques\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "overall_valid =  baseline.group_by('date').agg([pl.col(col).sum() for col in forecast_cols])\n",
    "overall_hist =  train_data.filter(\n",
    "            pl.col('date') >= pl.datetime(2021, 1, 1), \n",
    "            pl.col('date') < subset['date'].min()\n",
    "            ).sort(by='date').group_by('date').agg(pl.col(y).sum())\n",
    "\n",
    "overall_realise =  train_data.filter(\n",
    "            pl.col('date') >= subset['date'].min()\n",
    "            ).sort(by='date').group_by('date').agg(pl.col(y).sum())\n",
    "\n",
    "plt.figure(figsize=(25, 6))\n",
    "plt.scatter(\n",
    "    overall_hist['date'], overall_hist['y'], label=\"train\", marker=\"o\"\n",
    ")\n",
    "plt.scatter(\n",
    "    overall_realise['date'], overall_realise['y'], label=\"realise\", marker=\"o\"\n",
    ")\n",
    "for col in forecast_cols:\n",
    "    plt.scatter(\n",
    "        overall_valid['date'], overall_valid[col], label=f\"{col}_valid\", marker=\"*\"\n",
    "    )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
