{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import polars as pl\n",
    "from datetime import timedelta\n",
    "import datetime \n",
    "import json \n",
    "import toml\n",
    "import holidays\n",
    "import sys\n",
    "\n",
    "features = toml.load(r'C:\\Users\\N000193384\\Documents\\sncf_project\\sncf_playground\\data\\features.toml')\n",
    "times_cols = features['times_cols']\n",
    "macro_horizon = features['MACRO_HORIZON']\n",
    "p = Path(features['ABS_DATA_PATH'])\n",
    "sys.path.insert(1, p)\n",
    "\n",
    "from src.preprocessing.times import (\n",
    "    from_day_to_time_fe,\n",
    "    get_covid_table,\n",
    ")\n",
    "from src.preprocessing.quality import trim_timeseries, minimum_length_uid\n",
    "from src.models.forecast.direct import DirectForecaster\n",
    "from src.preprocessing.lags import get_significant_lags\n",
    "from src.preprocessing.times import get_basic_holidays\n",
    "from src.project_utils import load_data\n",
    "from src.models.lgb_wrapper import GBTModel\n",
    "from src.preprocessing.validation import freeze_validation_set\n",
    "from src.preprocessing.lags import compute_autoreg_features\n",
    "\n",
    "ts_uid = features[\"ts_uid\"]\n",
    "date_col = features['date_col']\n",
    "y = features['y']\n",
    "submit = False \n",
    "flist = features[\"flist\"]\n",
    "long_horizon = np.arange(macro_horizon)\n",
    "chains = np.array_split(long_horizon, 6)\n",
    "exog = [\"job\", \"ferie\", \"vacances\"] + times_cols\n",
    "\n",
    "with open('data/params.json', 'rb') as stream:\n",
    "    params_q = json.load(stream)\n",
    "\n",
    "in_dt = datetime.date(2020, 6, 1)\n",
    "\n",
    "covid_df = get_covid_table(2015, 2024)\n",
    "df_dates = df_dates = get_basic_holidays()\n",
    "holidays_fe = list(filter(lambda x: date_col not in x, df_dates.columns))\n",
    "covid_fe = list(filter(lambda x: date_col not in x, covid_df.columns))\n",
    "exog = exog + holidays_fe + covid_fe\n",
    "\n",
    "train_data, test_data, submission = load_data(p)\n",
    "\n",
    "test_data = (\n",
    "    test_data.pipe(from_day_to_time_fe, time=date_col, frequency=\"day\")\n",
    "    .join(df_dates, how=\"left\", on=[date_col])\n",
    "    .join(covid_df, how=\"left\", on=[date_col])\n",
    ")\n",
    "\n",
    "train_data = (\n",
    "    train_data.pipe(trim_timeseries, target=\"y\", uid=ts_uid, time=date_col)\n",
    "    .pipe(from_day_to_time_fe, time=date_col, frequency=\"day\")\n",
    "    .join(df_dates, how=\"left\", on=[date_col])\n",
    "    .join(\n",
    "        covid_df.with_columns(\n",
    "            pl.lit(np.where(np.any(covid_df != 0, axis=1), 0.01, 1)).alias(\n",
    "                \"covid_weight\"\n",
    "            )\n",
    "        ),\n",
    "        how=\"left\",\n",
    "        on=[date_col],\n",
    "    )\n",
    "    # sncf strike | 2019-12-01 to 2021-11-01\n",
    "    .filter(\n",
    "        (pl.col(date_col) >= in_dt)\n",
    "        & (pl.col(date_col) != pl.datetime(2019, 12, 1))\n",
    "        & (pl.col(date_col) != pl.datetime(2021, 11, 1))\n",
    "    )\n",
    ")\n",
    "\n",
    "good_ts = minimum_length_uid(\n",
    "    train_data, target=y, uid=ts_uid, time=date_col, min_length=round(364 * 1.2)\n",
    ")\n",
    "train_data = train_data.filter(pl.col(ts_uid).is_in(good_ts))\n",
    "# test.\n",
    "left_term = train_data.select([date_col, ts_uid, \"y\"] + exog).with_columns(\n",
    "    pl.lit(1).alias(\"train\")\n",
    ")\n",
    "right_term = test_data.select([date_col, ts_uid, \"y\"] + exog).with_columns(\n",
    "    pl.lit(0).alias(\"train\")\n",
    ")\n",
    "full_data = pl.concat((left_term, right_term), how=\"vertical_relaxed\")\n",
    "del left_term, right_term\n",
    "\n",
    "# define params\n",
    "significant_lags = get_significant_lags(train_data, date_col=date_col, target=y)\n",
    "significant_lags = [x for x in significant_lags if x%7 == 0 and x<= macro_horizon][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 104)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>station</th><th>job</th><th>ferie</th><th>vacances</th><th>index</th><th>is_train</th><th>y</th><th>y_copy</th><th>min_dt</th><th>year</th><th>week</th><th>month</th><th>day</th><th>day_of_year</th><th>day_of_week</th><th>quarter</th><th>week_of_month</th><th>is_weekend</th><th>Jour de l&#x27;an</th><th>Lundi de Pâques</th><th>Fête du Travail</th><th>Fête de la Victoire</th><th>Ascension</th><th>Lundi de Pentecôte</th><th>Fête nationale</th><th>Assomption</th><th>Toussaint</th><th>Armistice</th><th>Noël</th><th>First Lockdown</th><th>Second Lockdown</th><th>Third Lockdown (Partial)</th><th>covid_weight</th><th>ar_y_lag188_station</th><th>ar_y_lag195_station</th><th>ar_y_lag202_station</th><th>&hellip;</th><th>scaled_Fête du Travail</th><th>scaled_Fête de la Victoire</th><th>scaled_Ascension</th><th>scaled_Lundi de Pentecôte</th><th>scaled_Fête nationale</th><th>scaled_Assomption</th><th>scaled_Toussaint</th><th>scaled_Armistice</th><th>scaled_Noël</th><th>scaled_ar_y_lag188_station</th><th>scaled_ar_y_lag195_station</th><th>scaled_ar_y_lag202_station</th><th>scaled_ar_y_lag230_station</th><th>scaled_ar_y_lag251_station</th><th>scaled_ar_y_lag300_station</th><th>scaled_ar_y_win28_shift181_rolling_mean_station</th><th>scaled_ar_y_win28_shift181_rolling_median_station</th><th>scaled_ar_y_win28_shift181_rolling_std_station</th><th>scaled_ar_y_win28_shift181_rolling_skew_station</th><th>scaled_ar_y_win56_shift181_rolling_mean_station</th><th>scaled_ar_y_win56_shift181_rolling_median_station</th><th>scaled_ar_y_win56_shift181_rolling_std_station</th><th>scaled_ar_y_win56_shift181_rolling_skew_station</th><th>scaled_ar_y_lag28.0_station@day_of_week</th><th>scaled_ar_y_lag29.0_station@day_of_week</th><th>scaled_ar_y_lag30.0_station@day_of_week</th><th>scaled_ar_y_lag31.0_station@day_of_week</th><th>scaled_ar_y_lag32.0_station@day_of_week</th><th>scaled_ar_y_lag33.0_station@day_of_week</th><th>scaled_ar_y_win4_shift27_rolling_mean_station@day_of_week</th><th>scaled_ar_y_win4_shift27_rolling_median_station@day_of_week</th><th>scaled_ar_y_win4_shift27_rolling_std_station@day_of_week</th><th>scaled_ar_y_win4_shift27_rolling_skew_station@day_of_week</th><th>scaled_ar_y_win8_shift27_rolling_mean_station@day_of_week</th><th>scaled_ar_y_win8_shift27_rolling_median_station@day_of_week</th><th>scaled_ar_y_win8_shift27_rolling_std_station@day_of_week</th><th>scaled_ar_y_win8_shift27_rolling_skew_station@day_of_week</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>&hellip;</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 104)\n",
       "┌──────┬─────────┬─────┬───────┬───┬───────────────┬───────────────┬───────────────┬───────────────┐\n",
       "│ date ┆ station ┆ job ┆ ferie ┆ … ┆ scaled_ar_y_w ┆ scaled_ar_y_w ┆ scaled_ar_y_w ┆ scaled_ar_y_w │\n",
       "│ ---  ┆ ---     ┆ --- ┆ ---   ┆   ┆ in8_shift27_r ┆ in8_shift27_r ┆ in8_shift27_r ┆ in8_shift27_r │\n",
       "│ u32  ┆ u32     ┆ u32 ┆ u32   ┆   ┆ olli…         ┆ olli…         ┆ olli…         ┆ olli…         │\n",
       "│      ┆         ┆     ┆       ┆   ┆ ---           ┆ ---           ┆ ---           ┆ ---           │\n",
       "│      ┆         ┆     ┆       ┆   ┆ u32           ┆ u32           ┆ u32           ┆ u32           │\n",
       "╞══════╪═════════╪═════╪═══════╪═══╪═══════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
       "│ 0    ┆ 0       ┆ 0   ┆ 0     ┆ … ┆ 0             ┆ 0             ┆ 0             ┆ 0             │\n",
       "└──────┴─────────┴─────┴───────┴───┴───────────────┴───────────────┴───────────────┴───────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 104)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>station</th><th>job</th><th>ferie</th><th>vacances</th><th>index</th><th>is_train</th><th>y</th><th>y_copy</th><th>min_dt</th><th>year</th><th>week</th><th>month</th><th>day</th><th>day_of_year</th><th>day_of_week</th><th>quarter</th><th>week_of_month</th><th>is_weekend</th><th>Jour de l&#x27;an</th><th>Lundi de Pâques</th><th>Fête du Travail</th><th>Fête de la Victoire</th><th>Ascension</th><th>Lundi de Pentecôte</th><th>Fête nationale</th><th>Assomption</th><th>Toussaint</th><th>Armistice</th><th>Noël</th><th>First Lockdown</th><th>Second Lockdown</th><th>Third Lockdown (Partial)</th><th>covid_weight</th><th>ar_y_lag188_station</th><th>ar_y_lag195_station</th><th>ar_y_lag202_station</th><th>&hellip;</th><th>scaled_Fête du Travail</th><th>scaled_Fête de la Victoire</th><th>scaled_Ascension</th><th>scaled_Lundi de Pentecôte</th><th>scaled_Fête nationale</th><th>scaled_Assomption</th><th>scaled_Toussaint</th><th>scaled_Armistice</th><th>scaled_Noël</th><th>scaled_ar_y_lag188_station</th><th>scaled_ar_y_lag195_station</th><th>scaled_ar_y_lag202_station</th><th>scaled_ar_y_lag230_station</th><th>scaled_ar_y_lag251_station</th><th>scaled_ar_y_lag300_station</th><th>scaled_ar_y_win28_shift181_rolling_mean_station</th><th>scaled_ar_y_win28_shift181_rolling_median_station</th><th>scaled_ar_y_win28_shift181_rolling_std_station</th><th>scaled_ar_y_win28_shift181_rolling_skew_station</th><th>scaled_ar_y_win56_shift181_rolling_mean_station</th><th>scaled_ar_y_win56_shift181_rolling_median_station</th><th>scaled_ar_y_win56_shift181_rolling_std_station</th><th>scaled_ar_y_win56_shift181_rolling_skew_station</th><th>scaled_ar_y_lag28.0_station@day_of_week</th><th>scaled_ar_y_lag29.0_station@day_of_week</th><th>scaled_ar_y_lag30.0_station@day_of_week</th><th>scaled_ar_y_lag31.0_station@day_of_week</th><th>scaled_ar_y_lag32.0_station@day_of_week</th><th>scaled_ar_y_lag33.0_station@day_of_week</th><th>scaled_ar_y_win4_shift27_rolling_mean_station@day_of_week</th><th>scaled_ar_y_win4_shift27_rolling_median_station@day_of_week</th><th>scaled_ar_y_win4_shift27_rolling_std_station@day_of_week</th><th>scaled_ar_y_win4_shift27_rolling_skew_station@day_of_week</th><th>scaled_ar_y_win8_shift27_rolling_mean_station@day_of_week</th><th>scaled_ar_y_win8_shift27_rolling_median_station@day_of_week</th><th>scaled_ar_y_win8_shift27_rolling_std_station@day_of_week</th><th>scaled_ar_y_win8_shift27_rolling_skew_station@day_of_week</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>&hellip;</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 104)\n",
       "┌──────┬─────────┬─────┬───────┬───┬───────────────┬───────────────┬───────────────┬───────────────┐\n",
       "│ date ┆ station ┆ job ┆ ferie ┆ … ┆ scaled_ar_y_w ┆ scaled_ar_y_w ┆ scaled_ar_y_w ┆ scaled_ar_y_w │\n",
       "│ ---  ┆ ---     ┆ --- ┆ ---   ┆   ┆ in8_shift27_r ┆ in8_shift27_r ┆ in8_shift27_r ┆ in8_shift27_r │\n",
       "│ u32  ┆ u32     ┆ u32 ┆ u32   ┆   ┆ olli…         ┆ olli…         ┆ olli…         ┆ olli…         │\n",
       "│      ┆         ┆     ┆       ┆   ┆ ---           ┆ ---           ┆ ---           ┆ ---           │\n",
       "│      ┆         ┆     ┆       ┆   ┆ u32           ┆ u32           ┆ u32           ┆ u32           │\n",
       "╞══════╪═════════╪═════╪═══════╪═══╪═══════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
       "│ 0    ┆ 0       ┆ 0   ┆ 0     ┆ … ┆ 0             ┆ 0             ┆ 0             ┆ 0             │\n",
       "└──────┴─────────┴─────┴───────┴───┴───────────────┴───────────────┴───────────────┴───────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "autoreg_dict = {\n",
    "        ts_uid: {\n",
    "            \"groups\": ts_uid,\n",
    "            \"horizon\": lambda horizon: np.int32(horizon),\n",
    "            \"wins\": np.array([28, 56]),\n",
    "            \"shifts\": lambda horizon: np.int32([horizon]),\n",
    "            \"lags\": lambda horizon: np.array(significant_lags) + horizon,\n",
    "            \"funcs\": np.array(flist),\n",
    "        },\n",
    "        \"ts_uid_dow\": {\n",
    "            \"groups\": [ts_uid, \"day_of_week\"],\n",
    "            \"horizon\": lambda horizon: np.int32(np.ceil(horizon / 7) + 1),\n",
    "            \"wins\": np.array([4, 8]),\n",
    "            \"shifts\": lambda horizon: np.int32([np.ceil(horizon / 7) + 1]),\n",
    "            \"lags\": lambda horizon: np.arange(1, 7) + np.ceil(horizon / 7) + 1,\n",
    "            \"funcs\": np.array(flist),\n",
    "        }\n",
    "    }\n",
    "\n",
    "for key in autoreg_dict.keys():\n",
    "    autoreg_dict[key][\"horizon\"] = autoreg_dict[key][\"horizon\"](macro_horizon)\n",
    "    autoreg_dict[key][\"shifts\"] = autoreg_dict[key][\"shifts\"](macro_horizon)\n",
    "    autoreg_dict[key][\"lags\"] = autoreg_dict[key][\"lags\"](macro_horizon)\n",
    "\n",
    "train_set, val_set = freeze_validation_set(\n",
    "    ((train_data\n",
    "     .with_columns(pl.col('y').log1p().alias('y'))\n",
    "    )\n",
    "    .pipe(compute_autoreg_features, \n",
    "           target_col=\"y\", \n",
    "           date_str=date_col, \n",
    "           auto_reg_params=autoreg_dict\n",
    "           )\n",
    "    .fill_null(0)\n",
    "    .fill_nan(0)\n",
    "    .filter(pl.col(date_col) >=  datetime.datetime(2019, 1, 1))\n",
    "    ),\n",
    "    date=date_col, \n",
    "    val_size=181, \n",
    "    return_train=True\n",
    "    )\n",
    "\n",
    "\n",
    "num_cols =  [\"job\", \"ferie\", \"vacances\"] + holidays_fe + list(filter(lambda x : x.startswith('ar_'), train_set.columns))\n",
    "cat_cols = ['week', 'month', 'day_of_week', 'day']\n",
    "target = \"y\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler().set_output(transform='polars')\n",
    "# Fit and transform the selected columns\n",
    "scaled_values = sc.fit_transform(train_set.select(num_cols))\n",
    "scaled_test_values = sc.transform(val_set.select(num_cols))\n",
    "# Add the scaled data as new columns to the Polars DataFrame\n",
    "for i, col in enumerate(num_cols):\n",
    "    train_set = train_set.with_columns(pl.Series(f\"scaled_{col}\", scaled_values[col]))\n",
    "    val_set = val_set.with_columns(pl.Series(f\"scaled_{col}\", scaled_test_values[col]))\n",
    "\n",
    "scaled_num_cols = [f\"scaled_{c}\" for c in num_cols]\n",
    "display(train_set.null_count(), val_set.null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:54<1:29:39, 54.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.8006, Val Loss: 0.3884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [01:48<1:28:54, 54.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Train Loss: 0.5862, Val Loss: 0.3652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [02:47<1:31:15, 56.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Train Loss: 0.5719, Val Loss: 0.3846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [03:45<1:30:58, 56.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Train Loss: 0.5618, Val Loss: 0.4890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [04:38<1:28:08, 55.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Train Loss: 0.5569, Val Loss: 0.4039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [05:33<1:26:41, 55.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Train Loss: 0.5517, Val Loss: 0.4162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [06:28<1:25:38, 55.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Train Loss: 0.5496, Val Loss: 0.3916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [07:23<1:24:34, 55.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Train Loss: 0.5456, Val Loss: 0.4008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [08:18<1:23:29, 55.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Train Loss: 0.5433, Val Loss: 0.3934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [09:12<1:22:14, 54.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 0.5395, Val Loss: 0.3649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [10:07<1:21:33, 54.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Train Loss: 0.5375, Val Loss: 0.3969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [11:03<1:20:50, 55.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Train Loss: 0.5356, Val Loss: 0.3570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [11:58<1:19:59, 55.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Train Loss: 0.5345, Val Loss: 0.3838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [12:53<1:19:01, 55.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Train Loss: 0.5329, Val Loss: 0.4137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [13:49<1:18:13, 55.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Train Loss: 0.5316, Val Loss: 0.3752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [14:45<1:17:43, 55.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Train Loss: 0.5301, Val Loss: 0.3579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [15:41<1:16:57, 55.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Train Loss: 0.5292, Val Loss: 0.4117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [16:38<1:16:54, 56.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Train Loss: 0.5289, Val Loss: 0.4127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [17:39<1:17:35, 57.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Train Loss: 0.5281, Val Loss: 0.3582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [18:41<1:18:42, 59.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Train Loss: 0.5275, Val Loss: 0.3761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [19:44<1:19:11, 60.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Train Loss: 0.5268, Val Loss: 0.4447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [20:44<1:18:02, 59.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Train Loss: 0.5260, Val Loss: 0.3726\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from typing import List\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, dataset:pl.DataFrame, continuous_data:List[str], categorical_data:List[str], target:str):\n",
    "        self.continuous_data = dataset.select(continuous_data).to_numpy()\n",
    "        self.categorical_data = dataset.select(categorical_data).to_numpy()\n",
    "        self.targets = dataset.select(target).to_numpy().ravel()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x': torch.tensor(self.continuous_data[idx], dtype=torch.float32),\n",
    "            'categorical_features': torch.tensor(self.categorical_data[idx], dtype=torch.long),\n",
    "            'y': torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "class SmoothQLoss(nn.L1Loss):\n",
    "    constants = ['reduction', 'beta', 'qx']\n",
    "    def __str__(self):\n",
    "        return f\"{self.__class__.__name__}(beta={self.beta}, qx={self.qx})\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        size_average=None,\n",
    "        reduce=None,\n",
    "        reduction: str = 'mean',\n",
    "        qx: float = 0.5,\n",
    "        beta: float = 0.1,\n",
    "    ):\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "        self.beta = beta\n",
    "        self.qx = qx\n",
    "    \n",
    "    def forward(\n",
    "      self, \n",
    "      predict: torch.Tensor, target: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        m = 2.0 * self.qx - 1.0\n",
    "        shift = self.beta * m\n",
    "        if self.reduction == 'mean':\n",
    "            return (\n",
    "                F.smooth_l1_loss(\n",
    "                  target - shift, predict, reduction='mean', \n",
    "                  beta=self.beta\n",
    "                ) + m * torch.mean(target - predict - 0.5 * shift)\n",
    "            )\n",
    "        elif self.reduction == 'sum':\n",
    "            return (\n",
    "                F.smooth_l1_loss(\n",
    "                    target - shift, predict, reduction='sum', \n",
    "                    beta=self.beta\n",
    "                ) + m * torch.sum(target - predict - 0.5 * shift)\n",
    "            )\n",
    "        \n",
    "class TimeSeriesMLP(nn.Module):\n",
    "    def __init__(self, num_features, embedding_sizes, hidden_dim=64, output_dim=1):\n",
    "        super(TimeSeriesMLP, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories, size in embedding_sizes])\n",
    "        embedding_dim = sum(e.embedding_dim for e in self.embeddings)\n",
    "        input_dim = num_features + embedding_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "    def forward(self, x, categorical_features):\n",
    "        embedded = [embedding(categorical_features[:, i]) for i, embedding in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "        x = torch.cat([x, embedded], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.squeeze(x) if self.output_dim == 1 else x\n",
    "\n",
    "train_ds = TimeSeriesDataset(train_set, \n",
    "    continuous_data=num_cols, \n",
    "    categorical_data=cat_cols, \n",
    "    target=target)\n",
    "\n",
    "val_ds = TimeSeriesDataset(val_set, \n",
    "    continuous_data=num_cols, \n",
    "    categorical_data=cat_cols, \n",
    "    target=target)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "# Example usage:\n",
    "num_lags = len(num_cols)\n",
    "num_categorical_features = len(cat_cols)\n",
    "unique_modalities = {col: train_data[col].n_unique() + 1 for col in cat_cols}\n",
    "embedding_sizes = [(nmodality, max(nmodality//4, 1)) for nmodality in unique_modalities.values()]\n",
    "model = TimeSeriesMLP(num_features=num_lags, embedding_sizes=embedding_sizes)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            x = batch['x']\n",
    "            y = batch['y']\n",
    "            categorical_features = batch['categorical_features']\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x, categorical_features)\n",
    "            loss = criterion(outputs.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch['x']\n",
    "                y = batch['y']\n",
    "                categorical_features = batch['categorical_features']\n",
    "                outputs = model(x, categorical_features)\n",
    "                loss = criterion(outputs.squeeze(), y)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (78_652, 57)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>station</th><th>y</th><th>job</th><th>ferie</th><th>vacances</th><th>week</th><th>month</th><th>day</th><th>day_of_week</th><th>quarter</th><th>week_of_month</th><th>is_weekend</th><th>Jour de l&#x27;an</th><th>Lundi de Pâques</th><th>Fête du Travail</th><th>Fête de la Victoire</th><th>Ascension</th><th>Lundi de Pentecôte</th><th>Fête nationale</th><th>Assomption</th><th>Toussaint</th><th>Armistice</th><th>Noël</th><th>First Lockdown</th><th>Second Lockdown</th><th>Third Lockdown (Partial)</th><th>train</th><th>ar_y_lag188_station</th><th>ar_y_lag195_station</th><th>ar_y_lag202_station</th><th>ar_y_lag230_station</th><th>ar_y_lag251_station</th><th>ar_y_lag300_station</th><th>ar_y_win28_shift181_rolling_mean_station</th><th>ar_y_win28_shift181_rolling_median_station</th><th>ar_y_win28_shift181_rolling_std_station</th><th>ar_y_win28_shift181_rolling_skew_station</th><th>ar_y_win56_shift181_rolling_mean_station</th><th>ar_y_win56_shift181_rolling_median_station</th><th>ar_y_win56_shift181_rolling_std_station</th><th>ar_y_win56_shift181_rolling_skew_station</th><th>ar_y_lag28.0_station@day_of_week</th><th>ar_y_lag29.0_station@day_of_week</th><th>ar_y_lag30.0_station@day_of_week</th><th>ar_y_lag31.0_station@day_of_week</th><th>ar_y_lag32.0_station@day_of_week</th><th>ar_y_lag33.0_station@day_of_week</th><th>ar_y_win4_shift27_rolling_mean_station@day_of_week</th><th>ar_y_win4_shift27_rolling_median_station@day_of_week</th><th>ar_y_win4_shift27_rolling_std_station@day_of_week</th><th>ar_y_win4_shift27_rolling_skew_station@day_of_week</th><th>ar_y_win8_shift27_rolling_mean_station@day_of_week</th><th>ar_y_win8_shift27_rolling_median_station@day_of_week</th><th>ar_y_win8_shift27_rolling_std_station@day_of_week</th><th>ar_y_win8_shift27_rolling_skew_station@day_of_week</th><th>y_hat</th></tr><tr><td>date</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>i32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2023-06-12</td><td>&quot;003&quot;</td><td>0.0</td><td>1</td><td>0</td><td>0</td><td>24</td><td>6</td><td>12</td><td>1</td><td>2</td><td>2</td><td>0</td><td>-162</td><td>-55</td><td>-42</td><td>-35</td><td>-17</td><td>-6</td><td>32</td><td>64</td><td>142</td><td>152</td><td>-169</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>6.50279</td><td>6.444131</td><td>6.523562</td><td>6.190315</td><td>6.327937</td><td>5.669881</td><td>6.123243</td><td>6.3926</td><td>0.503429</td><td>-1.157499</td><td>5.970889</td><td>6.202462</td><td>0.539507</td><td>-0.735262</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>571.546435</td></tr><tr><td>2023-01-16</td><td>&quot;003&quot;</td><td>0.0</td><td>1</td><td>0</td><td>0</td><td>3</td><td>1</td><td>16</td><td>1</td><td>1</td><td>3</td><td>0</td><td>-15</td><td>84</td><td>105</td><td>112</td><td>122</td><td>133</td><td>179</td><td>-154</td><td>-76</td><td>-66</td><td>-22</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>5.958425</td><td>6.202536</td><td>6.302619</td><td>6.380123</td><td>6.415097</td><td>6.378426</td><td>5.491885</td><td>5.700393</td><td>0.780794</td><td>-2.109293</td><td>5.698871</td><td>5.812636</td><td>0.676915</td><td>-2.03004</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>283.179948</td></tr><tr><td>2023-04-17</td><td>&quot;003&quot;</td><td>0.0</td><td>1</td><td>0</td><td>0</td><td>16</td><td>4</td><td>17</td><td>1</td><td>2</td><td>3</td><td>0</td><td>-106</td><td>1</td><td>14</td><td>21</td><td>31</td><td>42</td><td>88</td><td>120</td><td>-167</td><td>-157</td><td>-113</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>6.383507</td><td>6.327937</td><td>6.052089</td><td>6.047372</td><td>5.497168</td><td>6.388561</td><td>5.867098</td><td>6.042588</td><td>0.526752</td><td>-0.852274</td><td>5.690566</td><td>5.922917</td><td>0.68685</td><td>-1.996619</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.762051</td><td>5.762051</td><td>0.0</td><td>0.0</td><td>5.762051</td><td>5.762051</td><td>0.0</td><td>0.0</td><td>301.04559</td></tr><tr><td>2023-02-13</td><td>&quot;003&quot;</td><td>0.0</td><td>1</td><td>0</td><td>0</td><td>7</td><td>2</td><td>13</td><td>1</td><td>1</td><td>2</td><td>0</td><td>-43</td><td>56</td><td>77</td><td>84</td><td>94</td><td>105</td><td>151</td><td>-182</td><td>-104</td><td>-94</td><td>-50</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>5.497168</td><td>5.799093</td><td>5.937536</td><td>6.302619</td><td>6.455199</td><td>6.403574</td><td>5.478854</td><td>5.601294</td><td>0.354124</td><td>-0.572389</td><td>5.485369</td><td>5.655937</td><td>0.600735</td><td>-2.288161</td><td>6.418365</td><td>5.762051</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>6.003121</td><td>5.828946</td><td>0.361164</td><td>0.679914</td><td>6.003121</td><td>5.828946</td><td>0.361164</td><td>0.679914</td><td>434.628933</td></tr><tr><td>2023-05-08</td><td>&quot;003&quot;</td><td>0.0</td><td>1</td><td>1</td><td>1</td><td>19</td><td>5</td><td>8</td><td>1</td><td>2</td><td>2</td><td>0</td><td>-127</td><td>-20</td><td>-7</td><td>0</td><td>10</td><td>21</td><td>67</td><td>99</td><td>177</td><td>-178</td><td>-134</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>5.153292</td><td>6.190315</td><td>5.720312</td><td>5.929589</td><td>6.047372</td><td>5.958425</td><td>5.832029</td><td>6.060998</td><td>0.476577</td><td>-0.525255</td><td>5.824091</td><td>5.995145</td><td>0.511528</td><td>-0.749416</td><td>5.817111</td><td>0.0</td><td>5.828946</td><td>6.418365</td><td>5.762051</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>6.123655</td><td>0.0</td><td>0.0</td><td>58.744744</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2023-03-19</td><td>&quot;ZXY&quot;</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>11</td><td>3</td><td>19</td><td>7</td><td>1</td><td>3</td><td>1</td><td>-77</td><td>22</td><td>43</td><td>50</td><td>60</td><td>71</td><td>117</td><td>149</td><td>-138</td><td>-128</td><td>-84</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>8.8371</td><td>8.742893</td><td>8.412277</td><td>8.231642</td><td>8.534444</td><td>8.736489</td><td>8.319894</td><td>8.476619</td><td>0.561147</td><td>-0.942057</td><td>8.194059</td><td>8.260742</td><td>0.473778</td><td>-0.552283</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.577634</td><td>7.667626</td><td>6.269096</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.842137</td><td>0.0</td><td>0.0</td><td>445.725986</td></tr><tr><td>2023-02-05</td><td>&quot;ZXY&quot;</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>5</td><td>2</td><td>5</td><td>7</td><td>1</td><td>1</td><td>1</td><td>-35</td><td>64</td><td>85</td><td>92</td><td>102</td><td>113</td><td>159</td><td>-174</td><td>-96</td><td>-86</td><td>-42</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>8.231642</td><td>8.453401</td><td>8.563313</td><td>8.67163</td><td>8.724207</td><td>8.74767</td><td>8.104626</td><td>8.307072</td><td>0.498925</td><td>-1.512387</td><td>8.22353</td><td>8.414379</td><td>0.490266</td><td>-1.322743</td><td>7.089243</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.577634</td><td>7.667626</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.62263</td><td>0.0</td><td>0.0</td><td>236.889672</td></tr><tr><td>2023-04-23</td><td>&quot;ZXY&quot;</td><td>0.0</td><td>0</td><td>0</td><td>1</td><td>16</td><td>4</td><td>23</td><td>7</td><td>2</td><td>4</td><td>1</td><td>-112</td><td>-5</td><td>8</td><td>15</td><td>25</td><td>36</td><td>82</td><td>114</td><td>-173</td><td>-163</td><td>-119</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>8.742095</td><td>8.824678</td><td>8.78722</td><td>8.742893</td><td>7.634337</td><td>8.640472</td><td>8.392358</td><td>8.766311</td><td>0.667068</td><td>-1.131476</td><td>8.405891</td><td>8.756996</td><td>0.618954</td><td>-1.167601</td><td>5.808142</td><td>8.173575</td><td>7.075809</td><td>0.0</td><td>6.964136</td><td>7.595387</td><td>7.177755</td><td>7.364652</td><td>1.017221</td><td>-0.539396</td><td>0.0</td><td>7.335598</td><td>0.0</td><td>0.0</td><td>417.850365</td></tr><tr><td>2023-02-12</td><td>&quot;ZXY&quot;</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>6</td><td>2</td><td>12</td><td>7</td><td>1</td><td>2</td><td>1</td><td>-42</td><td>57</td><td>78</td><td>85</td><td>95</td><td>106</td><td>152</td><td>-181</td><td>-103</td><td>-93</td><td>-49</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>8.225771</td><td>8.231642</td><td>8.453401</td><td>8.640472</td><td>7.982416</td><td>7.707512</td><td>8.116615</td><td>8.251919</td><td>0.358013</td><td>-0.834421</td><td>8.165026</td><td>8.314341</td><td>0.483034</td><td>-1.211918</td><td>6.952729</td><td>8.097731</td><td>4.905275</td><td>6.139885</td><td>0.0</td><td>7.653495</td><td>0.0</td><td>7.52523</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.303112</td><td>0.0</td><td>0.0</td><td>281.220116</td></tr><tr><td>2023-05-28</td><td>&quot;ZXY&quot;</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>21</td><td>5</td><td>28</td><td>7</td><td>2</td><td>4</td><td>1</td><td>-147</td><td>-40</td><td>-27</td><td>-20</td><td>-2</td><td>1</td><td>47</td><td>79</td><td>157</td><td>167</td><td>-154</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>8.834483</td><td>8.851663</td><td>8.829665</td><td>8.824678</td><td>8.827761</td><td>8.231642</td><td>8.362384</td><td>8.717119</td><td>0.641156</td><td>-0.964931</td><td>8.388668</td><td>8.722969</td><td>0.60526</td><td>-1.064772</td><td>7.777793</td><td>7.398786</td><td>6.942157</td><td>7.499977</td><td>0.0</td><td>6.952729</td><td>6.941046</td><td>7.170471</td><td>0.928843</td><td>-0.717105</td><td>0.0</td><td>7.449381</td><td>0.0</td><td>0.0</td><td>571.167348</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (78_652, 57)\n",
       "┌────────────┬─────────┬─────┬─────┬───┬───────────────┬───────────────┬──────────────┬────────────┐\n",
       "│ date       ┆ station ┆ y   ┆ job ┆ … ┆ ar_y_win8_shi ┆ ar_y_win8_shi ┆ ar_y_win8_sh ┆ y_hat      │\n",
       "│ ---        ┆ ---     ┆ --- ┆ --- ┆   ┆ ft27_rolling_ ┆ ft27_rolling_ ┆ ift27_rollin ┆ ---        │\n",
       "│ date       ┆ str     ┆ f64 ┆ i64 ┆   ┆ medi…         ┆ std_…         ┆ g_skew…      ┆ f64        │\n",
       "│            ┆         ┆     ┆     ┆   ┆ ---           ┆ ---           ┆ ---          ┆            │\n",
       "│            ┆         ┆     ┆     ┆   ┆ f64           ┆ f64           ┆ f64          ┆            │\n",
       "╞════════════╪═════════╪═════╪═════╪═══╪═══════════════╪═══════════════╪══════════════╪════════════╡\n",
       "│ 2023-06-12 ┆ 003     ┆ 0.0 ┆ 1   ┆ … ┆ 0.0           ┆ 0.0           ┆ 0.0          ┆ 571.546435 │\n",
       "│ 2023-01-16 ┆ 003     ┆ 0.0 ┆ 1   ┆ … ┆ 0.0           ┆ 0.0           ┆ 0.0          ┆ 283.179948 │\n",
       "│ 2023-04-17 ┆ 003     ┆ 0.0 ┆ 1   ┆ … ┆ 5.762051      ┆ 0.0           ┆ 0.0          ┆ 301.04559  │\n",
       "│ 2023-02-13 ┆ 003     ┆ 0.0 ┆ 1   ┆ … ┆ 5.828946      ┆ 0.361164      ┆ 0.679914     ┆ 434.628933 │\n",
       "│ 2023-05-08 ┆ 003     ┆ 0.0 ┆ 1   ┆ … ┆ 6.123655      ┆ 0.0           ┆ 0.0          ┆ 58.744744  │\n",
       "│ …          ┆ …       ┆ …   ┆ …   ┆ … ┆ …             ┆ …             ┆ …            ┆ …          │\n",
       "│ 2023-03-19 ┆ ZXY     ┆ 0.0 ┆ 0   ┆ … ┆ 7.842137      ┆ 0.0           ┆ 0.0          ┆ 445.725986 │\n",
       "│ 2023-02-05 ┆ ZXY     ┆ 0.0 ┆ 0   ┆ … ┆ 7.62263       ┆ 0.0           ┆ 0.0          ┆ 236.889672 │\n",
       "│ 2023-04-23 ┆ ZXY     ┆ 0.0 ┆ 0   ┆ … ┆ 7.335598      ┆ 0.0           ┆ 0.0          ┆ 417.850365 │\n",
       "│ 2023-02-12 ┆ ZXY     ┆ 0.0 ┆ 0   ┆ … ┆ 7.303112      ┆ 0.0           ┆ 0.0          ┆ 281.220116 │\n",
       "│ 2023-05-28 ┆ ZXY     ┆ 0.0 ┆ 0   ┆ … ┆ 7.449381      ┆ 0.0           ┆ 0.0          ┆ 571.167348 │\n",
       "└────────────┴─────────┴─────┴─────┴───┴───────────────┴───────────────┴──────────────┴────────────┘"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(model, x_test):\n",
    "    x_test = TimeSeriesDataset(\n",
    "            x_test,\n",
    "            continuous_data=num_cols,\n",
    "            categorical_data=cat_cols,\n",
    "            target=target,\n",
    "        )\n",
    "    forecast_loader = DataLoader(x_test, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in forecast_loader:\n",
    "            x = batch[\"x\"]\n",
    "            categorical_features = batch[\"categorical_features\"]\n",
    "            outputs = model(x, categorical_features)\n",
    "            predictions.append(outputs.item())\n",
    "    return predictions\n",
    "\n",
    "test_out = (full_data\n",
    "            .with_columns(pl.col('y').log1p().alias('y'))\n",
    "              .pipe(compute_autoreg_features, \n",
    "           target_col=\"y\", \n",
    "           date_str=date_col, \n",
    "           auto_reg_params=autoreg_dict\n",
    "           )\n",
    "    .fill_null(0)\n",
    "    .fill_nan(0)\n",
    "    .filter(pl.col('train') == 0)\n",
    ")\n",
    "output = test_out.with_columns(pl.lit(np.expm1(predict(model, test_out))).alias('y_hat'))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nfrom typing import Dict, List\\n\\nimport lightgbm as lgb\\nimport numpy as np\\nimport optuna\\nimport pandas as pd\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.model_selection import train_test_split\\n\\n\\ndef objective(\\n    trial: optuna.trial,\\n    train_data: pd.DataFrame,\\n    exog: List,\\n    seed: int = 12345,\\n):\\n    \"\"\"_summary_\\n\\n    Args:\\n        trial (optuna.trial): _description_\\n        train_x (pd.DataFrame): _description_\\n        test (pd.DataFrame): _description_\\n        features (List): _description_\\n        target (str, optional): _description_. Defaults to \"\".\\n        seed (int, optional): _description_. Defaults to 12345.\\n\\n    Returns:\\n        _type_: _description_\\n    \"\"\"\\n    optuna_params = {\\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\"]),\\n        \"objective\": trial.suggest_categorical(\\n            \"objective\", [\"regression\", \"huber\", \"regression_l1\", \"quantile\"]\\n        ),\\n        \"metric\": trial.suggest_categorical(\"metric\", [\"rmse\"]),\\n        \"alpha\": trial.suggest_categorical(\\n            \"alpha\",\\n            [0.5, 0.52, 0.55, 0.57, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.7, 0.69, 0.7],\\n        ),\\n        \"force_row_wise\": trial.suggest_categorical(\"force_row_wise\", [True, False]),\\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05, log=False),\\n        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 15),\\n        \"sub_row\": trial.suggest_categorical(\"sub_row\", [0.6, 0.7, 0.8, 1.0]),\\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\\n        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 4, log=True),\\n        \"min_child_samples\": trial.suggest_float(\\n            \"min_child_samples\", 20, 5000, log=False\\n        ),\\n        \"num_iterations\": trial.suggest_int(\\n            \"n_estimators\",\\n            200,\\n            3000,\\n        ),\\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 25, 800),\\n        \"max_bins\": trial.suggest_int(\"max_bins\", 24, 1000),\\n        \"min_data_in_bin\": trial.suggest_int(\"min_data_in_bin\", 25, 1000),\\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 1000),\\n        \"feature_fraction_seed\": trial.suggest_categorical(\\n            \"feature_fraction_seed\", [seed]\\n        ),\\n        \"bagging_seed\": trial.suggest_categorical(\"bagging_seed\", [seed]),\\n        \"seed\": trial.suggest_categorical(\"seed\", [seed]),\\n        \"verbose\": trial.suggest_categorical(\"verbose\", [-1]),\\n    }\\n\\n    horizon = 181\\n    lags = [x for x in significant_lags if x <= horizon]\\n    win_list =  [ x for x in significant_lags if x % 7 == 0 and x <= horizon][1:] \\n\\n    autoreg_dict = {\\n        ts_uid : {\\n            \\'groups\\' : ts_uid,\\n            \\'horizon\\': lambda horizon : np.int32(horizon),\\n            \\'wins\\' : np.array(win_list), \\n            \\'shifts\\' : lambda horizon : np.int32([horizon, horizon+28, horizon+56]), \\n            \\'lags\\' : lambda horizon : np.array(significant_lags) + horizon,\\n            \\'funcs\\' : np.array(flist)\\n        },\\n        \"ts_uid_dow\" : {\\n            \\'groups\\':[ts_uid, \\'day_of_week\\'],\\n            \\'horizon\\' : lambda horizon : np.int32(np.ceil(horizon /7) + 1),\\n            \\'wins\\' : np.array([4, 8, 12, 16, 20]), \\n            \\'shifts\\' : lambda horizon : np.int32([np.ceil(horizon /7) + 1, np.ceil(horizon /7) + 4]), \\n            \\'lags\\' : lambda horizon : np.arange(1, 7) + np.ceil(horizon/7)+1,\\n            \\'funcs\\' : np.array(flist)\\n        }\\n    }\\n\\n\\n    effect_m = GBTModel(params=optuna_params, \\n                        early_stopping_value=200, \\n                        features= None,\\n                        custom_loss=optuna_params[\"objective\"], \\n                        categorical_features=[]\\n                        )\\n\\n\\n    dir_forecaster = DirectForecaster(\\n        model=effect_m,\\n        ts_uid=ts_uid,\\n        forecast_range=np.arange(horizon),\\n        target_str=\"y\",\\n        date_str=\"date\",\\n        exogs=exog,\\n        features_params=autoreg_dict\\n    )\\n\\n    dir_forecaster.fit(train_data=train_data)\\n    return dir_forecaster.evaluate()[\"mae\"].values[0]\\n\\n\\ndef parameters_tuning(\\n    initial_params: Dict,\\n    tuning_objective,\\n    n_trials: int = 25,\\n    njobs: int = -1,\\n):\\n    \"\"\"parameter for tuning over sudy\\n\\n    Args:\\n        tuning_objective (_type_): _description_\\n        n_trials (int, optional): _description_. Defaults to 25.\\n\\n    Returns:\\n        _type_: _description_\\n\\n    example :\\n\\n    func = lambda trial: objective(trial=trial,\\n                                    train_x=train_x,\\n                                    test=residualised_test,\\n                                    covariates=covariates,\\n                                    target=y,\\n                                    seed=12345\\n                                    )\\n    study_df, best_params = parameters_tuning(tuning_objective=func, n_trials=25, initial_params={})\\n        print(best_params)\\n        print(study_df)\\n        study_df.to_csv(\\'bparamslgb_new.csv\\', sep=\"|\", index=False)\\n    \"\"\"\\n    study = optuna.create_study(direction=\"minimize\")\\n    # study.enqueue_trial(initial_params)\\n    study.optimize(tuning_objective, n_trials=n_trials, n_jobs=njobs)\\n    print(\"Number of finished trials:\", len(study.trials))\\n    print(\"Best trial:\", study.best_trial.params)\\n    study_df = study.trials_dataframe()\\n    return study_df, study.best_params\\n\\n\\nfunc = lambda trial: objective(trial=trial,\\n                                train_data=train_data,\\n                                exog=exog,\\n                                seed=12345\\n                                )\\nstudy_df, best_params = parameters_tuning(tuning_objective=func, n_trials=30, initial_params={}, njobs=10)\\nprint(best_params)\\nprint(study_df)\\nstudy_df.to_csv(\\'bparamslgb_new.csv\\', sep=\"|\", index=False)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def objective(\n",
    "    trial: optuna.trial,\n",
    "    train_data: pd.DataFrame,\n",
    "    exog: List,\n",
    "    seed: int = 12345,\n",
    "):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial): _description_\n",
    "        train_x (pd.DataFrame): _description_\n",
    "        test (pd.DataFrame): _description_\n",
    "        features (List): _description_\n",
    "        target (str, optional): _description_. Defaults to \"\".\n",
    "        seed (int, optional): _description_. Defaults to 12345.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    optuna_params = {\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\"]),\n",
    "        \"objective\": trial.suggest_categorical(\n",
    "            \"objective\", [\"regression\", \"huber\", \"regression_l1\", \"quantile\"]\n",
    "        ),\n",
    "        \"metric\": trial.suggest_categorical(\"metric\", [\"rmse\"]),\n",
    "        \"alpha\": trial.suggest_categorical(\n",
    "            \"alpha\",\n",
    "            [0.5, 0.52, 0.55, 0.57, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.7, 0.69, 0.7],\n",
    "        ),\n",
    "        \"force_row_wise\": trial.suggest_categorical(\"force_row_wise\", [True, False]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05, log=False),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 15),\n",
    "        \"sub_row\": trial.suggest_categorical(\"sub_row\", [0.6, 0.7, 0.8, 1.0]),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 4, log=True),\n",
    "        \"min_child_samples\": trial.suggest_float(\n",
    "            \"min_child_samples\", 20, 5000, log=False\n",
    "        ),\n",
    "        \"num_iterations\": trial.suggest_int(\n",
    "            \"n_estimators\",\n",
    "            200,\n",
    "            3000,\n",
    "        ),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 25, 800),\n",
    "        \"max_bins\": trial.suggest_int(\"max_bins\", 24, 1000),\n",
    "        \"min_data_in_bin\": trial.suggest_int(\"min_data_in_bin\", 25, 1000),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 1000),\n",
    "        \"feature_fraction_seed\": trial.suggest_categorical(\n",
    "            \"feature_fraction_seed\", [seed]\n",
    "        ),\n",
    "        \"bagging_seed\": trial.suggest_categorical(\"bagging_seed\", [seed]),\n",
    "        \"seed\": trial.suggest_categorical(\"seed\", [seed]),\n",
    "        \"verbose\": trial.suggest_categorical(\"verbose\", [-1]),\n",
    "    }\n",
    "\n",
    "    horizon = 181\n",
    "    lags = [x for x in significant_lags if x <= horizon]\n",
    "    win_list =  [ x for x in significant_lags if x % 7 == 0 and x <= horizon][1:] \n",
    "\n",
    "    autoreg_dict = {\n",
    "        ts_uid : {\n",
    "            'groups' : ts_uid,\n",
    "            'horizon': lambda horizon : np.int32(horizon),\n",
    "            'wins' : np.array(win_list), \n",
    "            'shifts' : lambda horizon : np.int32([horizon, horizon+28, horizon+56]), \n",
    "            'lags' : lambda horizon : np.array(significant_lags) + horizon,\n",
    "            'funcs' : np.array(flist)\n",
    "        },\n",
    "        \"ts_uid_dow\" : {\n",
    "            'groups':[ts_uid, 'day_of_week'],\n",
    "            'horizon' : lambda horizon : np.int32(np.ceil(horizon /7) + 1),\n",
    "            'wins' : np.array([4, 8, 12, 16, 20]), \n",
    "            'shifts' : lambda horizon : np.int32([np.ceil(horizon /7) + 1, np.ceil(horizon /7) + 4]), \n",
    "            'lags' : lambda horizon : np.arange(1, 7) + np.ceil(horizon/7)+1,\n",
    "            'funcs' : np.array(flist)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    effect_m = GBTModel(params=optuna_params, \n",
    "                        early_stopping_value=200, \n",
    "                        features= None,\n",
    "                        custom_loss=optuna_params[\"objective\"], \n",
    "                        categorical_features=[]\n",
    "                        )\n",
    "\n",
    "\n",
    "    dir_forecaster = DirectForecaster(\n",
    "        model=effect_m,\n",
    "        ts_uid=ts_uid,\n",
    "        forecast_range=np.arange(horizon),\n",
    "        target_str=\"y\",\n",
    "        date_str=\"date\",\n",
    "        exogs=exog,\n",
    "        features_params=autoreg_dict\n",
    "    )\n",
    "\n",
    "    dir_forecaster.fit(train_data=train_data)\n",
    "    return dir_forecaster.evaluate()[\"mae\"].values[0]\n",
    "\n",
    "\n",
    "def parameters_tuning(\n",
    "    initial_params: Dict,\n",
    "    tuning_objective,\n",
    "    n_trials: int = 25,\n",
    "    njobs: int = -1,\n",
    "):\n",
    "    \"\"\"parameter for tuning over sudy\n",
    "\n",
    "    Args:\n",
    "        tuning_objective (_type_): _description_\n",
    "        n_trials (int, optional): _description_. Defaults to 25.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "\n",
    "    example :\n",
    "\n",
    "    func = lambda trial: objective(trial=trial,\n",
    "                                    train_x=train_x,\n",
    "                                    test=residualised_test,\n",
    "                                    covariates=covariates,\n",
    "                                    target=y,\n",
    "                                    seed=12345\n",
    "                                    )\n",
    "    study_df, best_params = parameters_tuning(tuning_objective=func, n_trials=25, initial_params={})\n",
    "        print(best_params)\n",
    "        print(study_df)\n",
    "        study_df.to_csv('bparamslgb_new.csv', sep=\"|\", index=False)\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    # study.enqueue_trial(initial_params)\n",
    "    study.optimize(tuning_objective, n_trials=n_trials, n_jobs=njobs)\n",
    "    print(\"Number of finished trials:\", len(study.trials))\n",
    "    print(\"Best trial:\", study.best_trial.params)\n",
    "    study_df = study.trials_dataframe()\n",
    "    return study_df, study.best_params\n",
    "\n",
    "\n",
    "func = lambda trial: objective(trial=trial,\n",
    "                                train_data=train_data,\n",
    "                                exog=exog,\n",
    "                                seed=12345\n",
    "                                )\n",
    "study_df, best_params = parameters_tuning(tuning_objective=func, n_trials=30, initial_params={}, njobs=10)\n",
    "print(best_params)\n",
    "print(study_df)\n",
    "study_df.to_csv('bparamslgb_new.csv', sep=\"|\", index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MAE, MSE\n",
    "from neuralforecast.models import PatchTST, TSMixer, iTransformer\n",
    "from typing import List, Union, Dict, Tuple\n",
    "from src.analysis.metrics import display_metrics\n",
    "\n",
    "\n",
    "class NeuralWrapper:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: List,\n",
    "        ts_uid: str,\n",
    "        date_col: str,\n",
    "        target: str,\n",
    "        forecast_horizon: int,\n",
    "        fill_strategy: str = \"forward\",\n",
    "        frequency: str = \"1d\",\n",
    "        levels: List[int] = [95],\n",
    "        conformalised: bool = False,\n",
    "        fitted: bool = False,\n",
    "    ):\n",
    "        self.models = models\n",
    "        self.fill_strategy = fill_strategy\n",
    "        self.ts_uid = ts_uid\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.date_col = date_col\n",
    "        self.target = target\n",
    "        self.frequency = frequency\n",
    "        self.conformalised = conformalised\n",
    "        self.levels = levels\n",
    "        self.fitted = fitted\n",
    "        self.forecast_range = np.arange(self.forecast_horizon)\n",
    "\n",
    "    def fill_gap(self, x: pl.DataFrame, date_range):\n",
    "        return (\n",
    "            pl.DataFrame({self.ts_uid: x[self.ts_uid][0], self.date_col: date_range})\n",
    "            .join(x, on=self.date_col, how=\"left\")[\n",
    "                [self.ts_uid, self.target, self.date_col]\n",
    "            ]\n",
    "            .with_columns(pl.col(self.target).fill_null(strategy=self.fill_strategy))\n",
    "        )\n",
    "\n",
    "    def nixtla_reformat(self, Y_df: pl.DataFrame, date_col: str) -> pl.DataFrame:\n",
    "        # get min,  max date\n",
    "        mn, mx = (\n",
    "            Y_df[date_col].cast(pl.Date).min(),\n",
    "            Y_df[date_col].cast(pl.Date).max(),\n",
    "        )\n",
    "        # construct the range\n",
    "        r = pl.date_range(mn, mx, self.frequency, eager=True)\n",
    "        # group by \"id\" and fill the missing dates\n",
    "        Y_df = (\n",
    "            Y_df.group_by(self.ts_uid, maintain_order=True)\n",
    "            .apply(lambda x: self.fill_gap(x, r))\n",
    "            .rename({date_col: \"ds\", self.ts_uid: \"unique_id\", self.target: \"y\"})\n",
    "            .with_columns(pl.col(\"ds\").cast(pl.Date).alias(\"ds\"))\n",
    "        )\n",
    "        return Y_df\n",
    "\n",
    "    def ensemble(self, forecasts_df):\n",
    "        all_col = forecasts_df.columns\n",
    "        lb_cols = list(filter(lambda x: \"-lo-\" in x, all_col))\n",
    "        ub_cols = list(filter(lambda x: \"-hi-\" in x, all_col))\n",
    "        point_fcst_cols = list(\n",
    "            filter(lambda x: x not in [\"ds\", \"unique_id\"] + lb_cols + ub_cols, all_col)\n",
    "        )\n",
    "        if len(point_fcst_cols) > 1:\n",
    "            forecasts_df = forecasts_df.with_columns(\n",
    "                [\n",
    "                    pl.concat_list(point_fcst_cols)\n",
    "                    .list.mean()\n",
    "                    .alias(\"arithmetic_forecast_ensamble\"),\n",
    "                    pl.concat_list(lb_cols)\n",
    "                    .list.mean()\n",
    "                    .alias(\"arithmetic_lower_bound_ensamble\"),\n",
    "                    pl.concat_list(ub_cols)\n",
    "                    .list.mean()\n",
    "                    .alias(\"arithmetic_upper_bound_ensamble\"),\n",
    "                ]\n",
    "            )\n",
    "        self.point_fcst_cols = point_fcst_cols\n",
    "        return forecasts_df\n",
    "\n",
    "    def temporal_train_test_split(\n",
    "        self, data: pl.DataFrame, date_col: str\n",
    "    ) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        date_series = data[date_col].cast(pl.Date)\n",
    "        max_available_date = date_series.max()\n",
    "        self.min_available_date = date_series.min()\n",
    "        self.start_valid = max_available_date - timedelta(\n",
    "            days=int(max(self.forecast_range))\n",
    "        )\n",
    "        self.end_valid = max_available_date\n",
    "        # retrieve max date from data\n",
    "        train = data.filter(pl.col(date_col) < self.start_valid)\n",
    "        valid = data.filter(\n",
    "            pl.col(date_col).is_between(self.start_valid, self.end_valid, closed=\"both\")\n",
    "        )\n",
    "        return train, valid\n",
    "    \n",
    "    def fit(self, Y_df, val_size:int):\n",
    "        self.models.fit(Y_df, val_size=val_size)\n",
    "\n",
    "    def forecast(self):\n",
    "        forecasts_df = (\n",
    "            self.models.predict(\n",
    "                fitted=self.fitted,\n",
    "                level=self.levels,\n",
    "                h=self.forecast_horizon,\n",
    "            )\n",
    "            .pipe(self.ensemble)\n",
    "        )\n",
    "        return forecasts_df\n",
    "\n",
    "    def evaluate_on_valid(self, Y_df):\n",
    "        train, valid = self.temporal_train_test_split(Y_df, date_col=\"ds\")\n",
    "        forecast_valid = self.forecast(self.fit(train))\n",
    "        metrics = pl.DataFrame()\n",
    "        for col in self.point_fcst_cols:\n",
    "            metrics.append(display_metrics(valid[self.target], forecast_valid[col]))\n",
    "        return valid, metrics\n",
    "\n",
    "\n",
    "sf =NeuralForecast([\n",
    "    PatchTST(h=macro_horizon, input_size=120, max_steps=1000, early_stop_patience_steps=3),\n",
    "    TSMixer(h=macro_horizon, input_size=120, n_series=439, max_steps=1000, early_stop_patience_steps=3),\n",
    "    iTransformer(h=macro_horizon, input_size=120, n_series=439, max_steps=1000, early_stop_patience_steps=3),\n",
    "], freq=\"1d\")\n",
    "\n",
    "nf_base = NeuralWrapper(\n",
    "    models=sf,\n",
    "    ts_uid=ts_uid,\n",
    "    date_col=date_col,\n",
    "    target=y,\n",
    "    forecast_horizon=macro_horizon,\n",
    "    fill_strategy=\"forward\",\n",
    "    frequency=\"1d\",\n",
    "    levels=[95],\n",
    "    conformalised=False,\n",
    "    fitted=False)\n",
    "\n",
    "\n",
    "train_data, test_data, submission = load_data(p)\n",
    "train_data = (\n",
    "    train_data.pipe(trim_timeseries, target=\"y\", uid=\"station\", time=\"date\")\n",
    "    .pipe(from_day_to_time_fe, time=\"date\", frequency=\"day\")\n",
    "    .filter(\n",
    "        (pl.col(\"date\") >= datetime.datetime(2017, 1, 1))\n",
    "        & (pl.col(\"date\") != pl.datetime(2019, 12, 1))\n",
    "        & (pl.col(\"date\") != pl.datetime(2021, 11, 1))\n",
    "    )\n",
    "    .pipe(nf_base.nixtla_reformat, date_col=\"date\")\n",
    "    .drop_nulls()\n",
    ")\n",
    "\n",
    "nf_base.fit(train_data, val_size=macro_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_output = nf_base.models.predict()\n",
    "nf_output = (nf_output.with_columns(\n",
    "    pl.concat_list(list(filter(lambda x : x not in ['unique_id', 'ds'], nf_output.columns))).list.mean().alias('ensamble_transformer'),\n",
    "    pl.col('ds').cast(pl.Date).alias(\"date\")\n",
    ").rename({\"unique_id\":\"station\"})\n",
    ".select([\"date\", \"station\", \"ensamble_transformer\"])\n",
    ")\n",
    "\n",
    "test_data_out = (test_data\n",
    " .with_columns(pl.col('date').cast(pl.Date))\n",
    " .join(nf_output, how=\"left\", on=[\"date\", \"station\"])\n",
    ").drop('y').rename({\"ensamble_transformer\":\"y\"}).select('index', 'y')\n",
    "\n",
    "test_data_out.write_csv('out/submit/neural_model_fcst.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>fname</th>\n",
       "      <th>reference_y</th>\n",
       "      <th>global_direct_y_hat</th>\n",
       "      <th>local_direct_y_hat</th>\n",
       "      <th>global_chain_y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fname</th>\n",
       "      <td>reference_y</td>\n",
       "      <td>global_direct_y_hat</td>\n",
       "      <td>local_direct_y_hat</td>\n",
       "      <td>global_chain_y_hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>4096.972498</td>\n",
       "      <td>2281.123641</td>\n",
       "      <td>13035.708215</td>\n",
       "      <td>2276.585432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <td>428.170286</td>\n",
       "      <td>152.05768</td>\n",
       "      <td>1211.907011</td>\n",
       "      <td>183.988699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mape</th>\n",
       "      <td>1.906594</td>\n",
       "      <td>1.698225</td>\n",
       "      <td>13.245355</td>\n",
       "      <td>1.792233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wfiab</th>\n",
       "      <td>0.710736</td>\n",
       "      <td>0.843646</td>\n",
       "      <td>0.199865</td>\n",
       "      <td>0.841658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>1357.987948</td>\n",
       "      <td>737.750658</td>\n",
       "      <td>5088.93654</td>\n",
       "      <td>747.963718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smape</th>\n",
       "      <td>43.227671</td>\n",
       "      <td>24.40726</td>\n",
       "      <td>122.868208</td>\n",
       "      <td>27.516662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "fname  reference_y  global_direct_y_hat  local_direct_y_hat  \\\n",
       "fname  reference_y  global_direct_y_hat  local_direct_y_hat   \n",
       "rmse   4096.972498          2281.123641        13035.708215   \n",
       "bias    428.170286            152.05768         1211.907011   \n",
       "mape      1.906594             1.698225           13.245355   \n",
       "wfiab     0.710736             0.843646            0.199865   \n",
       "mae    1357.987948           737.750658          5088.93654   \n",
       "smape    43.227671             24.40726          122.868208   \n",
       "\n",
       "fname  global_chain_y_hat  \n",
       "fname  global_chain_y_hat  \n",
       "rmse          2276.585432  \n",
       "bias           183.988699  \n",
       "mape             1.792233  \n",
       "wfiab            0.841658  \n",
       "mae            747.963718  \n",
       "smape           27.516662  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nwith log :\\n\\nfname\\tbaseline\\tlgb\\tlgb_chains\\nfname\\tbaseline\\tlgb\\tlgb_chains\\nrmse\\t4123.810547\\t4999.597825\\t5121.101499\\nbias\\t450.242163\\t1372.905182\\t1368.364562\\nmape\\t2.105475\\t1.154826\\t1.152257\\nwfiab\\t0.691066\\t0.650323\\t0.651813\\nmae\\t1359.627197\\t1538.938512\\t1532.378134\\n\\nwithout log : \\n\\nfname\\tbaseline\\tlgb\\tlgb_chains\\nfname\\tbaseline\\tlgb\\tlgb_chains\\nrmse\\t4139.15247\\t2512.594178\\t2364.075077\\nbias\\t421.788203\\t227.739412\\t235.493706\\nmape\\t2.774925\\t1.806896\\t1.947053\\nwfiab\\t0.68463\\t0.797759\\t0.806641\\nmae\\t1387.952732\\t890.065842\\t850.979446\\n\\nwo log | w covid \\n\\nfname\\tbaseline\\tlgb_chains\\nfname\\tbaseline\\tlgb_chains\\nrmse\\t4139.15247\\t2292.385397\\nbias\\t421.788203\\t229.864649\\nmape\\t2.774925\\t2.033086\\nwfiab\\t0.68463\\t0.80966\\nmae\\t1387.952732\\t837.692923\\n\\nfname\\tbaseline\\tlgb\\tlgb_chains\\nfname\\tbaseline\\tlgb\\tlgb_chains\\nrmse\\t4139.15247\\t2150.431433\\t2273.18081\\nbias\\t421.788203\\t101.213962\\t159.685819\\nmape\\t2.774925\\t2.723164\\t2.632403\\nwfiab\\t0.68463\\t0.808726\\t0.807924\\nmae\\t1387.952732\\t841.800341\\t845.329485\\n\\n\\nfname\\tbaseline\\tlgb\\tlgb_chains\\nfname\\tbaseline\\tlgb\\tlgb_chains\\nrmse\\t4143.447587\\t2214.968268\\t2127.142468\\nbias\\t462.428658\\t70.666715\\t109.148351\\nmape\\t1.934116\\t1.718658\\t1.738316\\nwfiab\\t0.692717\\t0.8319\\t0.834986\\nmae\\t1365.34212\\t746.9146\\t733.203599\\n\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_col = [ts_uid, date_col, y, \"day_of_year\"]\n",
    "\n",
    "from src.preprocessing.lags import reference_shift_from_day\n",
    "from src.analysis.metrics import display_metrics\n",
    "\n",
    "def freeze_validation_set(\n",
    "    df: pl.DataFrame,\n",
    "    date: str,\n",
    "    val_size: int,\n",
    "    return_train: bool = True,\n",
    ") -> pl.DataFrame:\n",
    "    max_dt = df[date].max()\n",
    "    cut = max_dt - timedelta(days=val_size)\n",
    "    valid = df.filter(pl.col(date) > cut)  # .select([ts_uid, date, target])\n",
    "    if return_train:\n",
    "        train = df.filter(pl.col(date) <= cut)\n",
    "        return train, valid\n",
    "    else:\n",
    "        return valid\n",
    "\n",
    "\n",
    "\"\"\"statsmodel_valid = (pl.read_csv('out/nixtla_validation.csv', separator=\",\", infer_schema_length=1000)\n",
    "                    .rename({\"unique_id\":ts_uid, \"ds\":date_col})\n",
    "                    .select([ts_uid, date_col, 'HoltWinters', 'AutoETS', \"AutoARIMA\",\n",
    "                              'AutoTheta', 'AutoTBATS', 'arithmetic_forecast_ensamble']\n",
    "                              ).with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "direct_global_valid = (pl.read_csv('out/global_direct_lgb.csv', separator=\",\", infer_schema_length=1000)\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    ")\n",
    "\n",
    "# neural_fcst_model = (pl.read_csv('out/global_direct_lgb.csv', separator=\",\", infer_schema_length=1000)\n",
    "#                      .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    "# )\n",
    "\n",
    "\n",
    "direct_local_valid = (pl.read_csv('out/local_direct_lgb.csv', separator=\",\", infer_schema_length=1000)\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    ")\n",
    "\n",
    "\n",
    "chain_global_valid = (pl.read_csv('out/global_chain_lgb.csv', separator=\",\", infer_schema_length=1000)\n",
    "                     # .rename({\"y_hat\":\"global_chain_y_hat\"})\n",
    "                     .with_columns(pl.col(date_col).cast(pl.Date).alias(date_col))\n",
    ")\n",
    "\n",
    "baseline = (train_data\n",
    "    .pipe(from_day_to_time_fe, time=\"date\", frequency=\"day\")\n",
    "    .pipe(reference_shift_from_day,    \n",
    "        target_col=y, \n",
    "        ts_uid=ts_uid,\n",
    "        dayofyear_col=\"day_of_year\"       )\n",
    "    .pipe(freeze_validation_set, return_train=False, date=date_col, val_size=181)\n",
    "    .with_columns(\n",
    "        pl.coalesce(\n",
    "            pl.col('reference_y'), \n",
    "            pl.col('reference_y').mean().over(ts_uid),\n",
    "            pl.col('reference_y').mean(),\n",
    "            0\n",
    "            )\n",
    "    )\n",
    "    # .join( statsmodel_valid, how=\"left\",  on=[ts_uid, date_col]    )\n",
    "    .join(\n",
    "            direct_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            direct_local_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    .join(\n",
    "            chain_global_valid,\n",
    "            how=\"left\", \n",
    "            on=[ts_uid, date_col]\n",
    "        )\n",
    "    )\n",
    "\n",
    "forecast_cols = [\n",
    "                '''\n",
    "                'HoltWinters', 'AutoETS', \"AutoARIMA\",\n",
    "                 'AutoTheta', 'AutoTBATS', 'AutoCES',\n",
    "                 'arithmetic_forecast_ensamble',\n",
    "                 '''\n",
    "                 \"reference_y\", \"global_direct_y_hat\", \n",
    "                 \"local_direct_y_hat\", \"global_chain_y_hat\"\n",
    "                 ]\n",
    "\n",
    "forecast_cols = [\n",
    "    \"reference_y\", \"global_direct_y_hat\", \n",
    "    \"local_direct_y_hat\", \"global_chain_y_hat\"\n",
    "]\n",
    "\n",
    "metrics_output_df = []\n",
    "for col in forecast_cols:\n",
    "    metrics_output_df.append(\n",
    "        display_metrics(\n",
    "        baseline[y].to_numpy(), \n",
    "        baseline[col].fill_null(0).to_numpy(),\n",
    "        name=col\n",
    "        ).transpose()\n",
    "    )\n",
    "\n",
    "allmet = pd.concat(metrics_output_df, axis=1)\n",
    "allmet.columns = allmet.iloc[0, :]\n",
    "display(allmet)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "with log :\n",
    "\n",
    "fname\tbaseline\tlgb\tlgb_chains\n",
    "fname\tbaseline\tlgb\tlgb_chains\n",
    "rmse\t4123.810547\t4999.597825\t5121.101499\n",
    "bias\t450.242163\t1372.905182\t1368.364562\n",
    "mape\t2.105475\t1.154826\t1.152257\n",
    "wfiab\t0.691066\t0.650323\t0.651813\n",
    "mae\t1359.627197\t1538.938512\t1532.378134\n",
    "\n",
    "without log : \n",
    "\n",
    "fname\tbaseline\tlgb\tlgb_chains\n",
    "fname\tbaseline\tlgb\tlgb_chains\n",
    "rmse\t4139.15247\t2512.594178\t2364.075077\n",
    "bias\t421.788203\t227.739412\t235.493706\n",
    "mape\t2.774925\t1.806896\t1.947053\n",
    "wfiab\t0.68463\t0.797759\t0.806641\n",
    "mae\t1387.952732\t890.065842\t850.979446\n",
    "\n",
    "wo log | w covid \n",
    "\n",
    "fname\tbaseline\tlgb_chains\n",
    "fname\tbaseline\tlgb_chains\n",
    "rmse\t4139.15247\t2292.385397\n",
    "bias\t421.788203\t229.864649\n",
    "mape\t2.774925\t2.033086\n",
    "wfiab\t0.68463\t0.80966\n",
    "mae\t1387.952732\t837.692923\n",
    "\n",
    "fname\tbaseline\tlgb\tlgb_chains\n",
    "fname\tbaseline\tlgb\tlgb_chains\n",
    "rmse\t4139.15247\t2150.431433\t2273.18081\n",
    "bias\t421.788203\t101.213962\t159.685819\n",
    "mape\t2.774925\t2.723164\t2.632403\n",
    "wfiab\t0.68463\t0.808726\t0.807924\n",
    "mae\t1387.952732\t841.800341\t845.329485\n",
    "\n",
    "\n",
    "fname\tbaseline\tlgb\tlgb_chains\n",
    "fname\tbaseline\tlgb\tlgb_chains\n",
    "rmse\t4143.447587\t2214.968268\t2127.142468\n",
    "bias\t462.428658\t70.666715\t109.148351\n",
    "mape\t1.934116\t1.718658\t1.738316\n",
    "wfiab\t0.692717\t0.8319\t0.834986\n",
    "mae\t1365.34212\t746.9146\t733.203599\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (193_074, 147)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>station</th><th>job</th><th>ferie</th><th>vacances</th><th>index</th><th>is_train</th><th>y</th><th>y_copy</th><th>min_dt</th><th>year</th><th>week</th><th>month</th><th>day</th><th>day_of_year</th><th>day_of_week</th><th>quarter</th><th>week_of_month</th><th>is_weekend</th><th>Jour de l&#x27;an</th><th>Lundi de Pâques</th><th>Fête du Travail</th><th>Fête de la Victoire</th><th>Ascension</th><th>Lundi de Pentecôte</th><th>Fête nationale</th><th>Assomption</th><th>Toussaint</th><th>Armistice</th><th>Noël</th><th>First Lockdown</th><th>Second Lockdown</th><th>Third Lockdown (Partial)</th><th>covid_weight</th><th>reference_y</th><th>job_right</th><th>ferie_right</th><th>&hellip;</th><th>ar_y_win4_shift71_rolling_mean_station@day_of_week</th><th>ar_y_win4_shift71_rolling_median_station@day_of_week</th><th>ar_y_win4_shift71_rolling_std_station@day_of_week</th><th>ar_y_win4_shift71_rolling_skew_station@day_of_week</th><th>ar_y_win12_shift27_rolling_mean_station@day_of_week</th><th>ar_y_win12_shift27_rolling_median_station@day_of_week</th><th>ar_y_win12_shift27_rolling_std_station@day_of_week</th><th>ar_y_win12_shift27_rolling_skew_station@day_of_week</th><th>ar_y_win12_shift71_rolling_mean_station@day_of_week</th><th>ar_y_win12_shift71_rolling_median_station@day_of_week</th><th>ar_y_win12_shift71_rolling_std_station@day_of_week</th><th>ar_y_win12_shift71_rolling_skew_station@day_of_week</th><th>ar_y_lag28.0_station@week</th><th>ar_y_lag29.0_station@week</th><th>ar_y_lag30.0_station@week</th><th>ar_y_lag31.0_station@week</th><th>ar_y_lag32.0_station@week</th><th>ar_y_lag33.0_station@week</th><th>ar_y_win4_shift27_rolling_mean_station@week</th><th>ar_y_win4_shift27_rolling_median_station@week</th><th>ar_y_win4_shift27_rolling_std_station@week</th><th>ar_y_win4_shift27_rolling_skew_station@week</th><th>ar_y_win4_shift71_rolling_mean_station@week</th><th>ar_y_win4_shift71_rolling_median_station@week</th><th>ar_y_win4_shift71_rolling_std_station@week</th><th>ar_y_win4_shift71_rolling_skew_station@week</th><th>ar_y_win12_shift27_rolling_mean_station@week</th><th>ar_y_win12_shift27_rolling_median_station@week</th><th>ar_y_win12_shift27_rolling_std_station@week</th><th>ar_y_win12_shift27_rolling_skew_station@week</th><th>ar_y_win12_shift71_rolling_mean_station@week</th><th>ar_y_win12_shift71_rolling_median_station@week</th><th>ar_y_win12_shift71_rolling_std_station@week</th><th>ar_y_win12_shift71_rolling_skew_station@week</th><th>global_direct_y_hat</th><th>local_direct_y_hat</th><th>global_chain_y_hat</th></tr><tr><td>date</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>bool</td><td>i64</td><td>i64</td><td>datetime[μs]</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i16</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2022-08-01</td><td>&quot;1J7&quot;</td><td>1</td><td>0</td><td>1</td><td>&quot;2022-08-01_1J7&quot;</td><td>true</td><td>143</td><td>143</td><td>2015-01-01 00:00:00</td><td>2022</td><td>31</td><td>8</td><td>1</td><td>213</td><td>1</td><td>3</td><td>1</td><td>0</td><td>153</td><td>-105</td><td>-92</td><td>-85</td><td>-67</td><td>-56</td><td>-18</td><td>14</td><td>92</td><td>102</td><td>146</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>31.0</td><td>1</td><td>0</td><td>&hellip;</td><td>75.0</td><td>72.5</td><td>53.944416</td><td>0.100333</td><td>122.333333</td><td>125.5</td><td>73.741974</td><td>-0.162132</td><td>127.166667</td><td>124.0</td><td>64.286056</td><td>0.104489</td><td>114</td><td>35</td><td>165</td><td>29</td><td>146</td><td>168</td><td>123.75</td><td>139.5</td><td>65.703247</td><td>-0.599997</td><td>null</td><td>null</td><td>null</td><td>null</td><td>126.0</td><td>155.5</td><td>58.715028</td><td>-0.730361</td><td>null</td><td>null</td><td>null</td><td>null</td><td>109.911369</td><td>5944.997533</td><td>114.335355</td></tr><tr><td>2022-08-01</td><td>&quot;O2O&quot;</td><td>1</td><td>0</td><td>1</td><td>&quot;2022-08-01_O2O&quot;</td><td>true</td><td>25</td><td>25</td><td>2015-01-02 00:00:00</td><td>2022</td><td>31</td><td>8</td><td>1</td><td>213</td><td>1</td><td>3</td><td>1</td><td>0</td><td>153</td><td>-105</td><td>-92</td><td>-85</td><td>-67</td><td>-56</td><td>-18</td><td>14</td><td>92</td><td>102</td><td>146</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>21.0</td><td>1</td><td>0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>34.25</td><td>34.5</td><td>11.354815</td><td>-1.32925</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>44.821908</td><td>911.85959</td><td>43.459832</td></tr><tr><td>2022-08-01</td><td>&quot;NZV&quot;</td><td>1</td><td>0</td><td>1</td><td>&quot;2022-08-01_NZV&quot;</td><td>true</td><td>17</td><td>17</td><td>2015-01-05 00:00:00</td><td>2022</td><td>31</td><td>8</td><td>1</td><td>213</td><td>1</td><td>3</td><td>1</td><td>0</td><td>153</td><td>-105</td><td>-92</td><td>-85</td><td>-67</td><td>-56</td><td>-18</td><td>14</td><td>92</td><td>102</td><td>146</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>10.0</td><td>1</td><td>0</td><td>&hellip;</td><td>25.5</td><td>24.0</td><td>7.593857</td><td>0.632839</td><td>31.083333</td><td>32.0</td><td>7.633161</td><td>0.187612</td><td>24.25</td><td>24.5</td><td>13.798057</td><td>-0.224094</td><td>16</td><td>30</td><td>5</td><td>10</td><td>null</td><td>null</td><td>16.5</td><td>15.5</td><td>10.279429</td><td>0.331678</td><td>null</td><td>null</td><td>null</td><td>null</td><td>15.2</td><td>15.0</td><td>9.364828</td><td>0.694429</td><td>null</td><td>null</td><td>null</td><td>null</td><td>42.110113</td><td>107.278727</td><td>41.38109</td></tr><tr><td>2022-08-01</td><td>&quot;8QR&quot;</td><td>1</td><td>0</td><td>1</td><td>&quot;2022-08-01_8QR&quot;</td><td>true</td><td>31</td><td>31</td><td>2015-01-01 00:00:00</td><td>2022</td><td>31</td><td>8</td><td>1</td><td>213</td><td>1</td><td>3</td><td>1</td><td>0</td><td>153</td><td>-105</td><td>-92</td><td>-85</td><td>-67</td><td>-56</td><td>-18</td><td>14</td><td>92</td><td>102</td><td>146</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>26.0</td><td>1</td><td>0</td><td>&hellip;</td><td>65.0</td><td>67.5</td><td>14.854853</td><td>-0.517823</td><td>56.75</td><td>57.5</td><td>16.415763</td><td>-0.16896</td><td>60.5</td><td>63.0</td><td>15.168748</td><td>-0.910178</td><td>49</td><td>40</td><td>55</td><td>42</td><td>36</td><td>34</td><td>44.0</td><td>44.5</td><td>10.099505</td><td>-0.125541</td><td>null</td><td>null</td><td>null</td><td>null</td><td>39.083333</td><td>40.5</td><td>11.453132</td><td>-0.547184</td><td>null</td><td>null</td><td>null</td><td>null</td><td>63.168051</td><td>4742.463486</td><td>60.529118</td></tr><tr><td>2022-08-01</td><td>&quot;UMC&quot;</td><td>1</td><td>0</td><td>1</td><td>&quot;2022-08-01_UMC&quot;</td><td>true</td><td>73</td><td>73</td><td>2015-01-01 00:00:00</td><td>2022</td><td>31</td><td>8</td><td>1</td><td>213</td><td>1</td><td>3</td><td>1</td><td>0</td><td>153</td><td>-105</td><td>-92</td><td>-85</td><td>-67</td><td>-56</td><td>-18</td><td>14</td><td>92</td><td>102</td><td>146</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>59.0</td><td>1</td><td>0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>135.166667</td><td>136.0</td><td>49.171006</td><td>-0.189187</td><td>null</td><td>null</td><td>null</td><td>null</td><td>53</td><td>92</td><td>109</td><td>96</td><td>105</td><td>null</td><td>89.75</td><td>98.5</td><td>25.552234</td><td>-0.898418</td><td>null</td><td>null</td><td>null</td><td>null</td><td>93.333333</td><td>100.5</td><td>20.75251</td><td>-1.435903</td><td>null</td><td>null</td><td>null</td><td>null</td><td>116.300432</td><td>6437.932622</td><td>115.257472</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2022-12-31</td><td>&quot;V2P&quot;</td><td>0</td><td>0</td><td>1</td><td>&quot;2022-12-31_V2P&quot;</td><td>true</td><td>1227</td><td>1227</td><td>2017-06-22 00:00:00</td><td>2022</td><td>52</td><td>12</td><td>31</td><td>365</td><td>6</td><td>4</td><td>5</td><td>1</td><td>1</td><td>100</td><td>121</td><td>128</td><td>138</td><td>149</td><td>-170</td><td>-138</td><td>-60</td><td>-50</td><td>-6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>952.0</td><td>0</td><td>0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1163</td><td>1033</td><td>1615</td><td>1214</td><td>800</td><td>734</td><td>1249.0</td><td>1174.0</td><td>253.05072</td><td>0.903864</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1004.7</td><td>1098.0</td><td>334.518576</td><td>0.144998</td><td>null</td><td>null</td><td>null</td><td>null</td><td>839.610472</td><td>424.608962</td><td>860.877356</td></tr><tr><td>2022-12-31</td><td>&quot;N9K&quot;</td><td>0</td><td>0</td><td>1</td><td>&quot;2022-12-31_N9K&quot;</td><td>true</td><td>544</td><td>544</td><td>2017-07-01 00:00:00</td><td>2022</td><td>52</td><td>12</td><td>31</td><td>365</td><td>6</td><td>4</td><td>5</td><td>1</td><td>1</td><td>100</td><td>121</td><td>128</td><td>138</td><td>149</td><td>-170</td><td>-138</td><td>-60</td><td>-50</td><td>-6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>376.0</td><td>0</td><td>0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>306.3</td><td>262.0</td><td>193.221261</td><td>0.860935</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>324.182437</td><td>12.452375</td><td>280.504882</td></tr><tr><td>2022-12-31</td><td>&quot;N9K&quot;</td><td>0</td><td>0</td><td>1</td><td>&quot;2022-12-31_N9K&quot;</td><td>true</td><td>544</td><td>544</td><td>2017-07-01 00:00:00</td><td>2022</td><td>52</td><td>12</td><td>31</td><td>365</td><td>6</td><td>4</td><td>5</td><td>1</td><td>1</td><td>100</td><td>121</td><td>128</td><td>138</td><td>149</td><td>-170</td><td>-138</td><td>-60</td><td>-50</td><td>-6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>376.0</td><td>0</td><td>0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>306.3</td><td>262.0</td><td>193.221261</td><td>0.860935</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>324.182437</td><td>12.452375</td><td>240.333234</td></tr><tr><td>2022-12-31</td><td>&quot;N9K&quot;</td><td>0</td><td>0</td><td>1</td><td>&quot;2022-12-31_N9K&quot;</td><td>true</td><td>544</td><td>544</td><td>2017-07-01 00:00:00</td><td>2022</td><td>52</td><td>12</td><td>31</td><td>365</td><td>6</td><td>4</td><td>5</td><td>1</td><td>1</td><td>100</td><td>121</td><td>128</td><td>138</td><td>149</td><td>-170</td><td>-138</td><td>-60</td><td>-50</td><td>-6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>376.0</td><td>0</td><td>0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>306.3</td><td>262.0</td><td>193.221261</td><td>0.860935</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>324.182437</td><td>12.452375</td><td>410.665127</td></tr><tr><td>2022-12-31</td><td>&quot;N9K&quot;</td><td>0</td><td>0</td><td>1</td><td>&quot;2022-12-31_N9K&quot;</td><td>true</td><td>544</td><td>544</td><td>2017-07-01 00:00:00</td><td>2022</td><td>52</td><td>12</td><td>31</td><td>365</td><td>6</td><td>4</td><td>5</td><td>1</td><td>1</td><td>100</td><td>121</td><td>128</td><td>138</td><td>149</td><td>-170</td><td>-138</td><td>-60</td><td>-50</td><td>-6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.01</td><td>376.0</td><td>0</td><td>0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>306.3</td><td>262.0</td><td>193.221261</td><td>0.860935</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>324.182437</td><td>12.452375</td><td>320.218112</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (193_074, 147)\n",
       "┌────────────┬─────────┬─────┬───────┬───┬──────────────┬──────────────┬─────────────┬─────────────┐\n",
       "│ date       ┆ station ┆ job ┆ ferie ┆ … ┆ ar_y_win12_s ┆ global_direc ┆ local_direc ┆ global_chai │\n",
       "│ ---        ┆ ---     ┆ --- ┆ ---   ┆   ┆ hift71_rolli ┆ t_y_hat      ┆ t_y_hat     ┆ n_y_hat     │\n",
       "│ date       ┆ str     ┆ i64 ┆ i64   ┆   ┆ ng_ske…      ┆ ---          ┆ ---         ┆ ---         │\n",
       "│            ┆         ┆     ┆       ┆   ┆ ---          ┆ f64          ┆ f64         ┆ f64         │\n",
       "│            ┆         ┆     ┆       ┆   ┆ str          ┆              ┆             ┆             │\n",
       "╞════════════╪═════════╪═════╪═══════╪═══╪══════════════╪══════════════╪═════════════╪═════════════╡\n",
       "│ 2022-08-01 ┆ 1J7     ┆ 1   ┆ 0     ┆ … ┆ null         ┆ 109.911369   ┆ 5944.997533 ┆ 114.335355  │\n",
       "│ 2022-08-01 ┆ O2O     ┆ 1   ┆ 0     ┆ … ┆ null         ┆ 44.821908    ┆ 911.85959   ┆ 43.459832   │\n",
       "│ 2022-08-01 ┆ NZV     ┆ 1   ┆ 0     ┆ … ┆ null         ┆ 42.110113    ┆ 107.278727  ┆ 41.38109    │\n",
       "│ 2022-08-01 ┆ 8QR     ┆ 1   ┆ 0     ┆ … ┆ null         ┆ 63.168051    ┆ 4742.463486 ┆ 60.529118   │\n",
       "│ 2022-08-01 ┆ UMC     ┆ 1   ┆ 0     ┆ … ┆ null         ┆ 116.300432   ┆ 6437.932622 ┆ 115.257472  │\n",
       "│ …          ┆ …       ┆ …   ┆ …     ┆ … ┆ …            ┆ …            ┆ …           ┆ …           │\n",
       "│ 2022-12-31 ┆ V2P     ┆ 0   ┆ 0     ┆ … ┆ null         ┆ 839.610472   ┆ 424.608962  ┆ 860.877356  │\n",
       "│ 2022-12-31 ┆ N9K     ┆ 0   ┆ 0     ┆ … ┆ null         ┆ 324.182437   ┆ 12.452375   ┆ 280.504882  │\n",
       "│ 2022-12-31 ┆ N9K     ┆ 0   ┆ 0     ┆ … ┆ null         ┆ 324.182437   ┆ 12.452375   ┆ 240.333234  │\n",
       "│ 2022-12-31 ┆ N9K     ┆ 0   ┆ 0     ┆ … ┆ null         ┆ 324.182437   ┆ 12.452375   ┆ 410.665127  │\n",
       "│ 2022-12-31 ┆ N9K     ┆ 0   ┆ 0     ┆ … ┆ null         ┆ 324.182437   ┆ 12.452375   ┆ 320.218112  │\n",
       "└────────────┴─────────┴─────┴───────┴───┴──────────────┴──────────────┴─────────────┴─────────────┘"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.with_columns(\n",
    "    pl.concat_list([\"reference_y\", \"global_direct_y_hat\", \"global_chain_y_hat\"]).list.mean().alias('ensemble'),\n",
    "    pl.concat_list(forecast_cols).list.mean().alias('all_ensemble'),\n",
    "    pl.concat_list([\"global_direct_y_hat\", \"global_chain_y_hat\"]).list.mean().alias('lgb_ensemble'),                   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "uids = baseline['station'].unique()\n",
    "n = 6\n",
    "choice = np.random.choice(uids, (n, ))\n",
    "\n",
    "# Création de la figure et des sous-graphiques\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 16))\n",
    "axs = axs.ravel()  # Aplatir le tableau 2D des axes pour itération facile\n",
    "\n",
    "# Tracer les graphiques\n",
    "for i, uid in enumerate(uids, start=0):\n",
    "    if i  < n :\n",
    "        subset_tr = train_data.filter(\n",
    "            pl.col(ts_uid) == uid, \n",
    "            pl.col('date') >= pl.datetime(2021, 1, 1), \n",
    "            pl.col('date') < subset['date'].min()\n",
    "            ).sort(by='date')\n",
    "        subset = baseline.filter(pl.col(ts_uid) == uid).sort(by='date')\n",
    "        axs[i].plot(subset_tr[\"date\"], subset_tr['y_copy'], label=\"train\", color=\"brown\", alpha=0.8)\n",
    "        axs[i].scatter(subset[\"date\"], subset['y'], label=\"real\", color=\"blue\", alpha=1, marker=\"*\")\n",
    "        for col in list(filter(lambda x : 'y_hat' in x ,forecast_cols)):\n",
    "            axs[i].scatter(subset[\"date\"], subset[col], label=col, alpha=0.5, marker=\"x\")\n",
    "        axs[i].axvline(subset['date'].min())\n",
    "        axs[i].set_title(f'Graph for ID {uid}')\n",
    "        axs[i].legend()\n",
    "\n",
    "# Ajuster l'espacement entre les sous-graphiques\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "overall_valid =  baseline.group_by('date').agg([pl.col(col).sum() for col in forecast_cols])\n",
    "overall_hist =  train_data.filter(\n",
    "            pl.col('date') >= pl.datetime(2021, 1, 1), \n",
    "            pl.col('date') < subset['date'].min()\n",
    "            ).sort(by='date').group_by('date').agg(pl.col(y).sum())\n",
    "\n",
    "overall_realise =  train_data.filter(\n",
    "            pl.col('date') >= subset['date'].min()\n",
    "            ).sort(by='date').group_by('date').agg(pl.col(y).sum())\n",
    "\n",
    "plt.figure(figsize=(25, 6))\n",
    "plt.scatter(\n",
    "    overall_hist['date'], overall_hist['y'], label=\"train\", marker=\"o\"\n",
    ")\n",
    "plt.scatter(\n",
    "    overall_realise['date'], overall_realise['y'], label=\"realise\", marker=\"o\"\n",
    ")\n",
    "for col in forecast_cols:\n",
    "    plt.scatter(\n",
    "        overall_valid['date'], overall_valid[col], label=f\"{col}_valid\", marker=\"*\"\n",
    "    )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
